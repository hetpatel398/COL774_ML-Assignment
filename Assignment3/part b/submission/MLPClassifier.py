import numpy as np
import math

class sigmoid:
    '''
    Sigmoid activation function
    '''
    def f(x):
        return 1/(1+np.exp(-1*x))
    def df(x):
        return x*(1-x)

class ReLU:
    '''
    ReLU activation function
    '''
    def f(x):
        x_=x.copy()
        x_[x_<0]=0
        return x_
    def df(x):
        x_=x.copy()
        x_[x_>0]=1
        return x_

class MLP:
    '''
        Implementation of Multi-layer perceptron for multi-class classification

        Parameters:
        *******************

        input_size(Default:784) : [int]
            Dimenstions of input dataset, also neurons in input layer

        layers([100,]) : [list]
            List containing number of hidden neurons in hidden layers

        output_size(Default:26) : [int]
            Number of classes in output label

        batch_size(Default:100) : [int]
            Batch size for mini batch SGD

        lr(Default:0.1) : [float]
            Learning rate (Seed learning rate for adaptive learning rate)

        adaptive_lr(Default:False) : [boolean]
            Flag suggesting use of adaptive learning rate.
            This is like inverse scaling learning rate and
            in each epoch learning rate will be lr/sqrt(n_epoch)

        activation_fn(Default:'sigmoid') : [string]
            Activation function for neurons. Can pass either 'sigmoid'
            or 'relu'. Note that actiavtion function for neurons in
            output layer will always be sigmoid.

        max_epoch(Default:1000): [int]
            Maximum number of epoch that SGD can run

        tol(Default:1e-4) : [float]
            Tolerence that will be used for stopping criteria for SGD.

            If early_stopping=False:
                loss for n_iter_no_change is not reducing by tol then stop
            If early_stopping=True:
                validation accuracy for n_iter_no_change does not increase by tol then stop

        n_iter_no_change(Default:10) : [int]
            Used for stopping criteria for SGD

        initialization(Default:'glorot-uniform') : [string]
            Method to use for weight initialization

            Available methods:
            1. 'glorot-normal':
                Normal distribution with mean=0 and scale=sqrt(2/(fan_in+fan_out))
            2. 'glorot-uniform':
                Uniform distribution with low=sqrt(6/(fan_in+fan_out))
                and high=sqrt(6/(fan_in+fan_out))
            3. 'he-normal':
                Normal distribution with mean=0 and scale=sqrt(2/fan_in)
            4. 'he-uniform':
                Uniform distribution with low=sqrt(6/fan_in) and high=sqrt(6/fan_in)
            5. 'normal':
                Normal distribution with mean=0 and scale=0.05
            6. 'uniform':
                Uniform distribution with low=-0.05 and high=0.05

        early_stopping(Default:False) : [boolean]
            if Ture then set aside 10% of Train dataset as validation dataset
            and then use accuracy on that validation dataset in each epoch for
            stopping criteria for SGD

        loss_fn(Default:'squared-loss') : [string]
            Loss_function used for optimization problem
            Available options are 'squared-loss' and 'cross-entropy'

        Attributes:
        *******************

        layers : [list]
            List containg number of neurons in each layer including input and output layers
        n_layers : [int]
            number of layers in network including input and output layers
        weights : [list]
            list of length (n_layers-1) containing weight matrices
        intercepts : [list]
            list of length n_layers containing intercept lists
        loss_lst : [list]
            list of accumulated loss calculated in each epoch

        Methods:
        *******************

        fit(self, X, Y) : [None]
            This method trains MLP using batch SGD. If you feelmlike stopping training
            anytime and get the weights learned till that time just Interrupt and the
            training will be stopped(Here Y should be one-hot encoded matrix)

        predict(self, X) : [array]
            Returns predicted one-hot encoded output matrix generated by our model
            shape : (n_samples,n_classes)

        predict_proba(self, X) : [array]
            Returns probabilities generated for each class label by our model
            shape : (n_samples,n_classes)

        predict_log_proba(self, X) : [array]
            Returns log probabilities generated for each class label by our model
            shape : (n_samples,n_classes)

        score(self, X, Y) : [float]
            Returns accuracy over dataset X and ground truth Y
    '''
    def __init__(self, input_size=784, layers=[100,], output_size=26,\
                 batch_size=100, lr=0.1, adaptive_lr=False, activation_fn='sigmoid',\
                 max_epoch=1000, tol=1e-4, n_iter_no_change=10, initialization='glorot-uniform',\
                 early_stopping=False, loss_fn='squared-loss'):
        self.input_size=input_size;
        self.layers=layers;
        self.output_size=output_size;
        self.batch_size=batch_size;
        self.lr=lr
        self.adaptive_lr=adaptive_lr
        self.activation_fn=activation_fn
        self.max_epoch=max_epoch
        self.tol=tol
        self.n_iter_no_change=n_iter_no_change
        self.initialization = initialization
        self.early_stopping = early_stopping
        self.loss_fn = loss_fn

        if activation_fn=='sigmoid':
            self.activation_class = sigmoid
        elif activation_fn=='relu':
            self.activation_class = ReLU
        else:
            raise Exception("Enter valid activation_fn")

        self.layers.insert(0, self.input_size); self.layers.append(self.output_size)
        self.n_layers = len(self.layers)


    def __str__(self):
        return """MLP(input_size=%s, layers=%s, output_size=%s, batch_size=%s,
    lr=%s, adaptive_lr=%s, activation_fn=%s, max_epoch=%s, tol=%s,
    n_iter_no_change=%s, initialization=%s, early_stopping=%s, loss_fn=%s)"""%\
                (self.input_size, self.layers[1:-1], self.output_size,\
                self.batch_size, self.lr, self.adaptive_lr, self.activation_fn,\
                self.max_epoch, self.tol, self.n_iter_no_change, self.initialization,\
                self.early_stopping, self.loss_fn)

    def __repr__(self):
        return self.__str__()

    def fit(self, X, Y):
        '''
        This method trains MLP model using given train dataset using batch SGD
        '''
        np.random.seed(0)

        f = self.activation_class.f
        df = self.activation_class.df

        X_train, Y_train = X, Y

        #Generating Validation dataset for early stopping
        if self.early_stopping:
            val_indices = np.random.choice(X.shape[0], size=int(X.shape[0]*0.1))
            train_indices = np.setdiff1d(list(range(X.shape[0])), val_indices)
            X_train, Y_train = X[train_indices], Y[train_indices]
            X_val, Y_val = X[val_indices], Y[val_indices]

        cnt_no_change=0
        self.loss_lst=[np.inf]
        best_loss=np.inf
        best_val_score=0

        #Initializing weights and intercept terms
        self._intialize_weights(method=self.initialization)

        try:
            for self.n_epoch in (range(self.max_epoch)):
                loss_sum=0

                #Shuffling train dataset
                indices = np.random.choice(X_train.shape[0], size=X_train.shape[0])
                X_train_ = X_train[indices]
                Y_train_ = Y_train[indices]

                if self.adaptive_lr:
                    lr=self.lr/math.sqrt(self.n_epoch+1)
                else:
                    lr=self.lr

                for i in range(0, X_train.shape[0], self.batch_size):
                    X_=X_train_[i:i+self.batch_size,:]
                    Y_=Y_train_[i:i+self.batch_size,:]

                    # Forward-prop
                    output = self._forward_prop(X_)

                    # Back-Prop
                    memo, updates = self._back_prop(output, Y_)

                    #Computing loss
                    loss_sum+=self._compute_loss(X_, Y_)

                    # Update
                    for i in range(self.n_layers-1):
                        self.intercepts[i]=self.intercepts[i] - lr * np.sum(memo[i],axis=0)
                        self.weights[i]=self.weights[i] - lr * updates[i]

                loss=loss_sum/math.ceil(X_train.shape[0]/self.batch_size)
                self.loss_lst.append(loss)
#                 print(loss)

                #Checking for convergence
                if self.early_stopping:
                    val_score = self.score(X_val, Y_val)
                    if(val_score-best_val_score<self.tol):
                        cnt_no_change+=1
                    else:
                        cnt_no_change=0
                    if(val_score>best_val_score):
                        best_val_score=val_score
                else:
                    if(best_loss-self.loss_lst[-1]<self.tol):
                        cnt_no_change+=1
                    else:
                        cnt_no_change=0
                    if(self.loss_lst[-1]<best_loss):
                        best_loss=self.loss_lst[-1]

                if(cnt_no_change==self.n_iter_no_change):
                    print('Converged in %d epochs'%(self.n_epoch))
                    break

        except KeyboardInterrupt:
            print("Training interrupted by user after %d epochs"%(self.n_epoch))

    def _forward_prop(self,X):
        """
        Returns output generated by each layers for the given X
        """
        f = self.activation_class.f
        output=[X]
        for i in range(self.n_layers-2):
            output.append(f(np.dot(output[-1],self.weights[i])+self.intercepts[i]))
        output.append(sigmoid.f(np.dot(output[-1],self.weights[-1])+self.intercepts[-1]))
        return output

    def _back_prop(self, output, Y):
        '''
        Returns memoization i.e. deltas and gradients for update in SGD for each weight arrays
        '''
        df=self.activation_class.df

        memo = []
        updates = []
        if self.loss_fn=='squared-loss':
            curr_memo = (-1/self.batch_size)*(np.multiply(Y-output[-1],sigmoid.df(output[-1])))
        elif self.loss_fn=='cross-entropy':
            curr_memo = (-1/self.batch_size)*(Y-output[-1])
        memo.insert(0,curr_memo)
        curr_update = np.dot(output[-2].T, curr_memo)
        updates.insert(0,curr_update)
        for i in range(self.n_layers-2, 0, -1):
            curr_memo = np.multiply(memo[0].dot(self.weights[i].T),df(output[i]))
            memo.insert(0,curr_memo)
            curr_update = (output[i-1].T).dot(curr_memo)
            updates.insert(0,curr_update)

        return memo, updates

    def _compute_loss(self, X, Y):
        '''
        Returns loss value for given X and Y
        '''
        output = self._forward_prop(X)
        if self.loss_fn=='squared-loss':
            loss = (0.5/X.shape[0])*np.sum((Y-output[-1])**2)
        elif self.loss_fn=='cross-entropy':
            ones = np.where(Y==1)
            zeros = np.where(Y==0)
            loss = (1/X.shape[0])*(-np.sum(np.log(output[-1][ones]))-np.sum(np.log(1-output[-1][zeros])))

        return loss

    def _intialize_weights(self, method='glorot-normal'):
        '''
        Initalizes weights and intercepts using specified technique for weight initilization
        '''
        self.weights=[]
        self.intercepts=[]
        for i in range(self.n_layers-1):
            fan_in = self.layers[i]
            fan_out = self.layers[i+1]

            #Glorot/Xevier Normal initialization
            if method=='glorot-normal':
                self.weights.append(np.random.normal(scale=math.sqrt(2/(fan_in+fan_out)), size=(fan_in, fan_out)))
                self.intercepts.append(np.random.normal(scale=math.sqrt(2/(fan_in+fan_out)), size=fan_out))

            #He-Normal intilization
            elif method=='he-normal':
                self.weights.append(np.random.normal(scale=np.sqrt(2/fan_in), size=(fan_in, fan_out)))
                self.intercepts.append(np.random.normal(scale=np.sqrt(2/fan_in), size=fan_out))

            #Glorot/Xevier uniform initialization
            elif method=='glorot-uniform':
                self.weights.append(np.random.uniform(low=-math.sqrt(6/(fan_in+fan_out)),\
                                                      high=math.sqrt(6/(fan_in+fan_out)),\
                                                      size=(fan_in, fan_out)))
                self.intercepts.append(np.random.uniform(low=-math.sqrt(6/(fan_in+fan_out)),\
                                                         high=math.sqrt(6/(fan_in+fan_out)),\
                                                         size=fan_out))
            #He-uniform intilization
            elif method=='he-uniform':
                self.weights.append(np.random.uniform(low=-np.sqrt(6/fan_in),\
                                                      high=np.sqrt(6/fan_in),\
                                                      size=(fan_in, fan_out)))
                self.intercepts.append(np.random.uniform(low=-np.sqrt(6/fan_in),\
                                                         high=np.sqrt(6/fan_in),\
                                                         size=fan_out))
            #Simple normal distribution with fixed 0 mean & 0.05 std
            elif method=='normal':
                self.weights.append(np.random.normal(scale=0.05, size=(fan_in, fan_out)))
                self.intercepts.append(np.random.normal(scale=0.05, size=fan_out))
            #Simple uniform distribution with low=-0.05 and high=0.05
            elif method=='uniform':
                self.weights.append(np.random.uniform(low=-0.05,\
                                                      high=0.05,\
                                                      size=(fan_in, fan_out)))
                self.intercepts.append(np.random.uniform(low=-0.05,\
                                                         high=0.05,\
                                                         size=fan_out))
            else:
                raise Exception('Enter Correct Initalization method')


    def predict(self, X):
        """
        Returns prediction as one-hot encoded output for given Dataset
        """
        pred_proba=self.predict_proba(X)
        pred=np.zeros(pred_proba.shape, dtype=np.int8)
        pred[np.arange(pred.shape[0]), np.argmax(pred_proba, axis=1)] = 1
        return pred

    def predict_proba(self, X):
        """
        Returns probabity of 26 classes as (n_sample,26) array
        """
        f = self.activation_class.f

        pred_proba=X
        for i in range(self.n_layers-2):
            pred_proba=f(pred_proba@self.weights[i]+self.intercepts[i])
        pred_proba=sigmoid.f(pred_proba@self.weights[-1]+self.intercepts[-1])

        return pred_proba

    def predict_log_proba(self, X):
        """
        Returns log probabity of 26 classes as (n_sample,26) array
        """
        return np.log(self.predict_proba(X))

    def score(self,X,Y):
        """
        Returns accuracy score for given X and Y
        """
        pred = self.predict(X)
        y_true = np.argmax(Y, axis=1)
        y_pred = np.argmax(pred, axis=1)
        return (np.sum(y_true==y_pred)/y_true.shape[0])
