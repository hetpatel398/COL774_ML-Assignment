{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<01:03, 15.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000, 784) (13000,) (6500, 784) (6500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:10<00:00, 14.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 13  6 ...  3  4 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86.61538461538461"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_train = np.genfromtxt('data/train.csv', delimiter=',')\n",
    "data_test = np.genfromtxt('data/test.csv', delimiter=',')\n",
    "\n",
    "X_train, y_train = data_train[:,:-1]/255, data_train[:,-1]\n",
    "X_test, y_test = data_test[:,:-1]/255, data_test[:,-1]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,\n",
    "                 R=26,\n",
    "                 eta=0.1,\n",
    "                 hidden_layers=[50]):\n",
    "        self.R = R\n",
    "        self.eta = eta\n",
    "        self.n_neurons = hidden_layers + [self.R]\n",
    "        self.n_layers = len(self.n_neurons)\n",
    "        \n",
    "    def loss_func(self, y_, O_):\n",
    "        y_enc = self.one_hot_encoding(y_)\n",
    "        error = ( y_enc - O_ )\n",
    "        return np.square( np.linalg.norm(error) )\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x*(1-x)\n",
    "\n",
    "    def activation(self, data, func='sig'):\n",
    "        if func == 'sig':\n",
    "            return self.sigmoid(data)\n",
    "        return data\n",
    "\n",
    "    def one_hot_encoding(self, y_):\n",
    "        '''\n",
    "            Expects a numpy array\n",
    "        '''\n",
    "        enc = np.zeros((y_.shape[0], self.R), dtype=np.int8)\n",
    "        for i, y in enumerate(y_):\n",
    "            enc[i, int(y)] = 1\n",
    "        return enc\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, X_):\n",
    "        self.outputs = []\n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0:\n",
    "                output = self.activation(np.dot(X_, self.weights[i])+self.biases[i])\n",
    "            else :\n",
    "                output = self.activation(np.dot(self.outputs[-1], self.weights[i])+self.biases[i])\n",
    "            self.outputs.append(output)\n",
    "        \n",
    "    def calculate_delta(self, y_):\n",
    "        self.deltas = [] # Careful delta is reversed\n",
    "        for i in reversed(range(self.n_layers)):\n",
    "            if i == self.n_layers-1 : # Output Layer\n",
    "                y_enc = self.one_hot_encoding(y_)\n",
    "                error = y_enc - self.outputs[i]\n",
    "                delta = -1*error*self.sigmoid_derivative(self.outputs[i])\n",
    "            else:\n",
    "                error = np.dot( self.deltas[-1], self.weights[i+1].T)\n",
    "                delta = (error*self.sigmoid_derivative(self.outputs[i]))\n",
    "            self.deltas.append(delta)\n",
    "\n",
    "        self.deltas = self.deltas[::-1]\n",
    "        \n",
    "    def update_theta(self, X_):\n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0 : # First layer\n",
    "                update = np.dot(X_.T, self.deltas[i])\n",
    "            else :\n",
    "                update = np.dot(self.outputs[i-1].T, self.deltas[i])\n",
    "            self.weights[i] -= 0.1*update/100\n",
    "            self.biases[i] -= 0.1*np.sum(self.deltas[i], axis=0)/100\n",
    "    \n",
    "    def fit(self, X_, y_):\n",
    "        \n",
    "        self.N = X_.shape[1]\n",
    "        self.M = X_.shape[0]\n",
    "        \n",
    "        self._intialize_weights(type='glorot-normal')\n",
    "        \n",
    "        max_itr = 1000\n",
    "        b_size = 100\n",
    "        \n",
    "        indices = np.random.choice(X_.shape[0], size=X_.shape[0])\n",
    "        X_ = X_[indices]\n",
    "        y_ = y_[indices]\n",
    "\n",
    "        for i in tqdm(range(max_itr)):\n",
    "            for idx in range(0, self.M, b_size):\n",
    "                self.feed_forward(X_[idx:idx+b_size,:])\n",
    "                self.calculate_delta(y_[idx:idx+b_size])\n",
    "                self.update_theta(X_[idx:idx+b_size,:])\n",
    "                \n",
    "    def predict(self, X_):\n",
    "        self.feed_forward(X_)\n",
    "        return self.outputs[-1]\n",
    "    \n",
    "    def score(self, X_, y_):\n",
    "        y_pred = np.argmax(self.predict(X_), axis=1)\n",
    "        print(y_pred)\n",
    "        return 100*(y_pred == y_).sum()/y_.shape[0]\n",
    "    \n",
    "    def _intialize_weights(self, type='glorot-normal'):\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        self.layers=[784,50,26]\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i+1]\n",
    "            #Glorot/Xevier Normal initialization\n",
    "            if type=='glorot-normal':\n",
    "                self.weights.append(np.random.normal(scale=(2/(fan_in+fan_out)), size=(fan_in, fan_out)))\n",
    "                self.biases.append(np.random.normal(scale=2/(fan_in+fan_out), size=fan_out))\n",
    "            #He-Normal intilization\n",
    "            elif type=='he-normal':\n",
    "                self.weights.append(np.random.normal(scale=np.sqrt(2/fan_in), size=(fan_in, fan_out)))\n",
    "                self.biases.append(np.random.normal(scale=np.sqrt(2/fan_in), size=fan_out))\n",
    "            #Glorot/Xevier uniform initialization\n",
    "            elif type=='glorot-uniform':\n",
    "                self.weights.append(np.random.uniform(low=-2/(fan_in+fan_out),\\\n",
    "                                                      high=2/(fan_in+fan_out),\\\n",
    "                                                      size=(fan_in, fan_out)))\n",
    "                self.biases.append(np.random.uniform(low=-2/(fan_in+fan_out),\\\n",
    "                                                         high=2/(fan_in+fan_out),\\\n",
    "                                                         size=fan_out))\n",
    "            #He-uniform intilization\n",
    "            elif type=='he-uniform':\n",
    "                self.weights.append(np.random.uniform(low=-np.sqrt(2/fan_in),\\\n",
    "                                                      high=np.sqrt(2/fan_in),\\\n",
    "                                                      size=(fan_in, fan_out)))\n",
    "                self.biases.append(np.random.uniform(low=-np.sqrt(2/fan_in),\\\n",
    "                                                         high=np.sqrt(2/fan_in),\\\n",
    "                                                         size=fan_out))\n",
    "            elif type=='uniform':\n",
    "                self.weights.append(np.random.uniform(low=-0.025,\\\n",
    "                                                      high=0.025,\\\n",
    "                                                      size=(fan_in, fan_out)))\n",
    "                self.biases.append(np.random.uniform(low=-0.025,\\\n",
    "                                                         high=0.025,\\\n",
    "                                                         size=fan_out))\n",
    "    \n",
    "\n",
    "\n",
    "nn = NeuralNetwork(eta=0.1, hidden_layers=[50])\n",
    "nn.fit(X_train, y_train)\n",
    "nn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24  5 24 ...  7 15 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.44615384615385"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000, 784) (13000,) (6500, 784) (6500,)\n",
      "(784, 50)\n",
      "(1, 50)\n",
      "(50, 26)\n",
      "(1, 26)\n",
      "136.64449405670166\n",
      "[25 13  6 ...  3  4 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.70769230769231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = np.genfromtxt('data/train.csv', delimiter=',')\n",
    "data_test = np.genfromtxt('data/test.csv', delimiter=',')\n",
    "\n",
    "X_train, y_train = data_train[:,:-1]/255, data_train[:,-1]\n",
    "X_test, y_test = data_test[:,:-1]/255, data_test[:,-1]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,\n",
    "                 R=26,\n",
    "                 eta=0.1,\n",
    "                 hidden_layers=[50]):\n",
    "#         np.random.seed(25)\n",
    "        \n",
    "        self.R = R\n",
    "        self.eta = eta\n",
    "        self.n_neurons = hidden_layers + [self.R]\n",
    "        self.n_layers = len(self.n_neurons)\n",
    "        \n",
    "    def loss_func(self, y_, O_):\n",
    "        y_enc = self.one_hot_encoding(y_)\n",
    "        error = ( y_enc - O_ )\n",
    "        return np.square( np.linalg.norm(error) )\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1.0+np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x*(1.0-x)\n",
    "\n",
    "    def activation(self, data, func='sig'):\n",
    "        if func == 'sig':\n",
    "            return self.sigmoid(data)\n",
    "        return data\n",
    "\n",
    "    def one_hot_encoding(self, y_):\n",
    "        '''\n",
    "            Expects a numpy array\n",
    "        '''\n",
    "        enc = np.zeros((y_.shape[0], self.R))\n",
    "        for i, y in enumerate(y_):\n",
    "            enc[i, int(y)] = 1.0\n",
    "        return enc\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, X_):\n",
    "        self.outputs = []\n",
    "        m = X_.shape[0]\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0 :\n",
    "                output = self.activation( np.dot( X_, self.weights[i]) + self.biases[i] )\n",
    "           \n",
    "            else :\n",
    "                output = self.activation( np.dot( self.outputs[-1], self.weights[i]) + self.biases[i] )\n",
    "                \n",
    "            self.outputs.append(output)\n",
    "#             print(output.shape)\n",
    "\n",
    "        \n",
    "    def calculate_delta(self, y_):\n",
    "        self.deltas = [] # Careful delta is reversed\n",
    "        for i in reversed(range(self.n_layers)):\n",
    "            \n",
    "            if i == self.n_layers-1 : # Output Layer\n",
    "                y_enc = self.one_hot_encoding(y_)\n",
    "                error = y_enc - self.outputs[i]\n",
    "                delta = error*self.sigmoid_derivative(self.outputs[i])\n",
    "\n",
    "            else:\n",
    "                error = np.dot( self.deltas[-1], self.weights[i+1].T )\n",
    "                delta = (error*self.sigmoid_derivative(self.outputs[i]))\n",
    "                \n",
    "            self.deltas.append(delta)\n",
    "#             print(delta.shape)\n",
    "\n",
    "        self.deltas = self.deltas[::-1]\n",
    "        \n",
    "    def update_theta(self, X_):\n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0 : # First layer\n",
    "                update = np.dot(X_.T, self.deltas[i])\n",
    "            else :\n",
    "                update = np.dot(self.outputs[i-1].T, self.deltas[i])\n",
    "            self.weights[i] += self.eta*update/self.deltas[i].shape[0]\n",
    "            self.biases[i] += self.eta*np.sum(self.deltas[i], axis=0)/self.deltas[i].shape[0]\n",
    "    \n",
    "    def fit(self, X_, y_):\n",
    "        self.N = X_.shape[1]\n",
    "        self.M = X_.shape[0]\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for neuron in self.n_neurons:\n",
    "            if self.weights == []:\n",
    "                self.weights.append( np.random.uniform(-0.1, 0.1,(self.N, neuron)) )\n",
    "            else:\n",
    "                self.weights.append( np.random.uniform(-0.1, 0.1,(self.weights[-1].shape[1], neuron)) )\n",
    "            self.biases.append( np.random.uniform(-0.1, 0.1,(1, neuron)) )\n",
    "            print(self.weights[-1].shape)\n",
    "            print(self.biases[-1].shape)\n",
    "        \n",
    "        prev_avg = np.inf\n",
    "        sum_loss = 0\n",
    "        total_itr = 0\n",
    "        max_itr = 2000*130\n",
    "        itr = 0 # For relooping over dataset\n",
    "        ITERATION_TO_EVAL_AT = 100000000\n",
    "        tol = 1e-40\n",
    "        b_size = 100\n",
    "        \n",
    "        while True:\n",
    "            for idx in range(0, self.M, b_size): # Possibly loss of last few training eg if not a multiple\n",
    "                if total_itr > max_itr:\n",
    "                    break\n",
    "                if itr == ITERATION_TO_EVAL_AT:\n",
    "                    if abs(sum_loss/itr - prev_avg) < tol:\n",
    "                        break\n",
    "                    prev_avg = sum_loss/itr\n",
    "                    itr = 0\n",
    "                    sum_loss = 0\n",
    "\n",
    "                self.feed_forward(X_[idx:idx+b_size,:])\n",
    "                self.calculate_delta(y_[idx:idx+b_size])\n",
    "                self.update_theta(X_[idx:idx+b_size,:])\n",
    "                \n",
    "#                 loss = self.loss_func(y_[idx:idx+b_size], self.outputs[-1])\n",
    "#                 print(loss)\n",
    "                sum_loss += 0\n",
    "                itr += 1\n",
    "                total_itr += 1\n",
    "\n",
    "            if total_itr > max_itr or abs(sum_loss/itr - prev_avg) < tol:\n",
    "                break\n",
    "            \n",
    "        \n",
    "    def predict(self, X_):\n",
    "        self.feed_forward(X_)\n",
    "        return self.outputs[-1]\n",
    "    \n",
    "    def score(self, X_, y_):\n",
    "        y_pred = np.argmax(self.predict(X_), axis=1)\n",
    "        print(y_pred)\n",
    "        return 100*(y_pred == y_).sum()/y_.shape[0]\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(eta=0.1, hidden_layers=[50])\n",
    "from time import time\n",
    "t0=time()\n",
    "nn.fit(X_train, y_train)\n",
    "print(time()-t0)\n",
    "nn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24  5 24 ...  7 15 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95.58461538461539"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
