{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv', header=None).to_numpy()\n",
    "test_data = pd.read_csv('data/test.csv', header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000, 785)\n",
      "(6500, 785)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[:,:-1]\n",
    "Y_train_orig = train_data[:,-1]\n",
    "X_test = test_data[:,:-1]\n",
    "Y_test_orig = test_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std=StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Y):\n",
    "    classes_ = np.unique(Y)\n",
    "    n_classes = classes_.shape[0]\n",
    "    Y_oh = np.zeros((Y.shape[0], n_classes), dtype=np.bool)\n",
    "    i=0\n",
    "    for y in Y:\n",
    "        Y_oh[i][y]=1\n",
    "        i+=1\n",
    "    return Y_oh\n",
    "\n",
    "Y_train = one_hot_encode(Y_train_orig)\n",
    "Y_test = one_hot_encode(Y_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[1].reshape((28,28)), cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.title('Example of train Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score : 0.9083076923076923\n",
      "Train score : 0.9999230769230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/het/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "clf_relu = MLPClassifier(hidden_layer_sizes=(100,), activation='relu')#, solver='sgd')\n",
    "\n",
    "clf_relu.fit(X_train_std, Y_train_orig)\n",
    "print('Test score :',clf_relu.score(X_test_std, Y_test_orig))\n",
    "print('Train score :',clf_relu.score(X_train_std, Y_train_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X,Y,pred):\n",
    "#     pred = self.predict(X)\n",
    "    y_true = np.argmax(Y, axis=1)\n",
    "    y_pred = np.argmax(pred, axis=1)\n",
    "    print(np.sum(y_true==y_pred)*100/y_true.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score : 0.8275384615384616\n",
      "Train score : 1.0\n",
      "85.64615384615385\n"
     ]
    }
   ],
   "source": [
    "clf_relu = MLPClassifier(hidden_layer_sizes=(100,), activation='relu')#, solver='sgd')\n",
    "\n",
    "clf_relu.fit(X_train_std, Y_train)\n",
    "print('Test score :',clf_relu.score(X_test_std, Y_test))\n",
    "print('Train score :',clf_relu.score(X_train_std, Y_train))\n",
    "pred_ = clf_relu.predict(X_test_std)\n",
    "score(X_test_std, Y_test, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.64615384615385\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(clf_relu.predict_proba(X_test_std), axis=1)\n",
    "score(X_test_std, Y_test, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8283076923076923"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_relu.score(X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    def f(x):\n",
    "        return 1/(1+np.exp(-1*x))\n",
    "    def df(x):\n",
    "        return x*(1-x)\n",
    "#         return sigmoid.f(x)*(1-sigmoid.f(x))\n",
    "    \n",
    "class ReLU:\n",
    "    def f(x):\n",
    "        x_=x.copy()\n",
    "        x_[x_<0]=0\n",
    "        return x_\n",
    "    def df(x):\n",
    "        x_=x.copy()\n",
    "        x_[x_<0]=0\n",
    "        x_[x_>0]=1\n",
    "        return x_\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, input_size=784, layers=[100,], output_size=26,\\\n",
    "                 batch_size=100, lr=0.1, adaptive_lr=False, activation_fn='sigmoid',\\\n",
    "                 tol=1e-4, n_iter_no_change=10, max_epoch=100):\n",
    "        self.input_size=input_size;\n",
    "        self.layers=layers;\n",
    "        self.output_size=output_size;\n",
    "        self.batch_size=batch_size;\n",
    "        self.lr=lr\n",
    "        self.adaptive_lr=adaptive_lr\n",
    "        \n",
    "        self.activation_fn=activation_fn\n",
    "        if activation_fn=='sigmoid':\n",
    "            self.activation_class = sigmoid\n",
    "        elif activation_fn=='relu':\n",
    "            self.activation_class = ReLU\n",
    "        else:\n",
    "            raise Exception(\"Enter valid activation_fn\")\n",
    "        \n",
    "        self.tol=tol\n",
    "        self.n_iter_no_change=n_iter_no_change \n",
    "        self.max_epoch=max_epoch\n",
    "        \n",
    "        self.layers.insert(0, self.input_size); self.layers.append(self.output_size)\n",
    "        self.n_layers = len(self.layers)\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"\"\"MLP(input_size=%s, layers=%s, output_size=%s,\n",
    "    batch_size=%s, lr=%s, adaptive_lr=%s, activation_fn=%s,\n",
    "    tol=%s, n_iter_no_change=%s, max_epoch=%s)\"\"\"%\\\n",
    "                (self.input_size, self.layers[1:-1], self.output_size,\\\n",
    "                self.batch_size, self.lr, self.adaptive_lr, self.activation_fn,\\\n",
    "                self.tol, self.n_iter_no_change, self.max_epoch)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        #Weights initialization\n",
    "        self._intialize_weights()\n",
    "        \n",
    "        f = self.activation_class.f\n",
    "        df = self.activation_class.df\n",
    "        loss_lst = np.array([])\n",
    "        for i in tqdm(range(self.max_epoch)):\n",
    "            indices = np.random.choice(X_train.shape[0], size=X_train.shape[0])\n",
    "            X_train_ = X_train[indices]\n",
    "            Y_train_ = Y_train[indices]\n",
    "            for i in range(0, X_train.shape[0], self.batch_size):\n",
    "                X=X_train_[i:i+self.batch_size,:]\n",
    "                Y=Y_train_[i:i+self.batch_size,:]  \n",
    "\n",
    "                # Forward-prop\n",
    "                output=[]\n",
    "                output.append(f(X@self.weights[0]+self.intercepts[0]))\n",
    "                for i in range(1,self.n_layers-2):\n",
    "                    output.append(f(output[i-1]@self.weights[i]+self.intercepts[i]))\n",
    "                output.append(sigmoid.f(output[-1]@self.weights[-1]+self.intercepts[-1]))\n",
    "\n",
    "                # Finding loss\n",
    "#                 out_=np.zeros(out[-1].shape, dtype=np.int8)\n",
    "#                 out_[np.arange(out_.shape[0]), np.argmax(out[-1], axis=1)] = 1\n",
    "\n",
    "#                 loss = (0.5/self.batch_size)*np.sum((out_-Y)**2)\n",
    "#                 loss = (0.5/self.batch_size)*np.sum((output[-1]-Y)**2)\n",
    "#                 loss_lst=np.append(loss_lst, loss)\n",
    "\n",
    "                # Back-Prop\n",
    "                memo = []\n",
    "                updates = []\n",
    "                curr_memo = (1/self.batch_size)*(np.multiply(Y-output[-1],sigmoid.df(output[-1])))\n",
    "                memo.insert(0,curr_memo)\n",
    "                curr_update = np.dot(output[-2].T, curr_memo)\n",
    "                updates.insert(0,curr_update)\n",
    "                for i in range(self.n_layers-2, 1, -1):\n",
    "                    curr_memo = (memo[0].dot(self.weights[i].T))*df(output[i-1])\n",
    "                    memo.insert(0,curr_memo)\n",
    "                    curr_update = output[i-2].T.dot(curr_memo)\n",
    "                    updates.insert(0,curr_update)\n",
    "                curr_memo = (memo[-1].dot(self.weights[1].T))*df(output[0])\n",
    "                memo.insert(0,curr_memo)\n",
    "                curr_update = X.T.dot(curr_memo)\n",
    "                updates.insert(0,curr_update)\n",
    "\n",
    "                self.intercepts[0]=self.intercepts[0] + self.lr*(memo[0].sum(axis=0))\n",
    "                self.intercepts[1]=self.intercepts[1] + self.lr*(memo[1].sum(axis=0))\n",
    "                self.weights[0] = self.weights[0] + self.lr*updates[0]\n",
    "                self.weights[1] = self.weights[1] + self.lr*updates[1]\n",
    "\n",
    "    \n",
    "    def _intialize_weights(self):\n",
    "        self.weights=[]\n",
    "        self.intercepts=[]\n",
    "        for i in range(self.n_layers-1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i+1]\n",
    "            self.weights.append(np.random.randn(fan_in, fan_out))\n",
    "            self.intercepts.append(np.random.randn(fan_out))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        pred_proba=self.predict_proba(X)\n",
    "        pred=np.zeros(pred_proba.shape, dtype=np.int8)\n",
    "        pred[np.arange(pred.shape[0]), np.argmax(pred_proba, axis=1)] = 1\n",
    "        return pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        f = self.activation_class.f\n",
    "        df = self.activation_class.df\n",
    "        \n",
    "        pred_proba=f(X@self.weights[0]+self.intercepts[0])\n",
    "        for i in range(1,self.n_layers-2):\n",
    "            pred_proba=f(pred_proba@self.weights[i]+self.intercepts[i])\n",
    "        pred_proba=sigmoid.f(pred_proba@self.weights[-1]+self.intercepts[-1])\n",
    "        \n",
    "        return pred_proba\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        return np.log(self.predict_proba(X))\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        pred = self.predict(X)\n",
    "        y_true = np.argmax(Y, axis=1)\n",
    "        y_pred = np.argmax(pred, axis=1)\n",
    "        print(np.sum(y_true==y_pred)*100/y_true.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 734/1000 [02:00<00:44,  5.98it/s]"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[100], batch_size=100, activation_fn='sigmoid', max_epoch=1000, )\n",
    "ml.fit(X_train/255, Y_train)\n",
    "ml.score(X_test/255, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.407692307692308\n"
     ]
    }
   ],
   "source": [
    "ml.score(X_train/255, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(self, X):\n",
    "    f = self.activation_class.f\n",
    "    df = self.activation_class.df\n",
    "\n",
    "    pred_proba=f(X@self.weights[0]+self.intercepts[0])\n",
    "    for i in range(1,self.n_layers-2):\n",
    "        pred_proba=f(pred_proba@self.weights[i]+self.intercepts[i])\n",
    "\n",
    "    pred_proba=sigmoid.f(pred_proba@self.weights[-1]+self.intercepts[-1])\n",
    "\n",
    "    return pred_proba\n",
    "def predict(self, X):\n",
    "    pred_proba=self.predict_proba(X)\n",
    "    pred=np.zeros(pred_proba.shape, dtype=np.int8)\n",
    "    pred[np.arange(pred.shape[0]), np.argmax(pred_proba, axis=1)] = 1\n",
    "\n",
    "    return pred\n",
    "def score(self,X,Y):\n",
    "    pred = self.predict(X)\n",
    "    y_true = np.argmax(Y, axis=1)\n",
    "    y_pred = np.argmax(pred, axis=1)\n",
    "    print(np.sum(y_true==y_pred)*100/y_true.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.37692307692308\n"
     ]
    }
   ],
   "source": [
    "ml.score(X_train/255, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(input_size=784, layers=[100], output_size=26,\n",
       "    batch_size=100, lr=0.1, adaptive_lr=False, activation_fn=sigmoid,\n",
       "    tol=0.0001, n_iter_no_change=10, max_iter=130000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='logistic', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train_std, Y_train_orig)\n",
    "print('Test score :',clf.score(X_test_std, Y_test_orig))\n",
    "print('Train score :',clf.score(X_train_std, Y_train_orig))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='logistic', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train_std, Y_train)\n",
    "print('Test score :',clf.score(X_test_std, Y_test))\n",
    "print('Train score :',clf.score(X_train_std, Y_train))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='logistic', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train, Y_train_orig)\n",
    "print('Test score :',clf.score(X_test, Y_test_orig))\n",
    "print('Train score :',clf.score(X_train, Y_train_orig))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='logistic', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "print('Test score :',clf.score(X_test, Y_test))\n",
    "print('Train score :',clf.score(X_train, Y_train))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='logistic', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train/255, Y_train_orig)\n",
    "print('Test score :',clf.score(X_test/255, Y_test_orig))\n",
    "print('Train score :',clf.score(X_train/255, Y_train_orig))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='logistic', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train/255, Y_train)\n",
    "print('Test score :',clf.score(X_test/255, Y_test))\n",
    "print('Train score :',clf.score(X_train/255, Y_train))\n",
    "print(clf.out_activation_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train_std, Y_train_orig)\n",
    "print('Test score :',clf.score(X_test_std, Y_test_orig))\n",
    "print('Train score :',clf.score(X_train_std, Y_train_orig))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train_std, Y_train)\n",
    "print('Test score :',clf.score(X_test_std, Y_test))\n",
    "print('Train score :',clf.score(X_train_std, Y_train))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train, Y_train_orig)\n",
    "print('Test score :',clf.score(X_test, Y_test_orig))\n",
    "print('Train score :',clf.score(X_train, Y_train_orig))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "print('Test score :',clf.score(X_test, Y_test))\n",
    "print('Train score :',clf.score(X_train, Y_train))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train/255, Y_train_orig)\n",
    "print('Test score :',clf.score(X_test/255, Y_test_orig))\n",
    "print('Train score :',clf.score(X_train/255, Y_train_orig))\n",
    "print(clf.out_activation_)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='sgd', tol=1e-3)\n",
    "\n",
    "clf.fit(X_train/255, Y_train)\n",
    "print('Test score :',clf.score(X_test/255, Y_test))\n",
    "print('Train score :',clf.score(X_train/255, Y_train))\n",
    "print(clf.out_activation_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
