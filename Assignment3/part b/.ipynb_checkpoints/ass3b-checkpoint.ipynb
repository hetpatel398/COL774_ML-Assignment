{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from time import time\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Y):\n",
    "    classes_ = np.unique(Y)\n",
    "    n_classes = classes_.shape[0]\n",
    "    Y_oh = np.zeros((Y.shape[0], n_classes), dtype=np.bool)\n",
    "    i=0\n",
    "    for y in Y:\n",
    "        Y_oh[i][y]=1\n",
    "        i+=1\n",
    "    return Y_oh\n",
    "\n",
    "train_data = pd.read_csv('data/train.csv', header=None).to_numpy()\n",
    "test_data = pd.read_csv('data/test.csv', header=None).to_numpy()\n",
    "\n",
    "X_train = train_data[:,:-1]/255\n",
    "Y_train_orig = train_data[:,-1]\n",
    "X_test = test_data[:,:-1]/255\n",
    "Y_test_orig = test_data[:,-1]\n",
    "\n",
    "Y_train = one_hot_encode(Y_train_orig)\n",
    "Y_test = one_hot_encode(Y_test_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    def f(x):\n",
    "        return 1/(1+np.exp(-1*x))\n",
    "    def df(x):\n",
    "        return x*(1-x)\n",
    "#         return sigmoid.f(x)*(1-sigmoid.f(x))\n",
    "    \n",
    "class ReLU:\n",
    "    def f(x):\n",
    "        x_=x.copy()\n",
    "        x_[x_<0]=0\n",
    "        return x_\n",
    "    \n",
    "    def df(x):\n",
    "        x_=x.copy()\n",
    "        x_[x_>0]=1\n",
    "        return x_\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, input_size=784, layers=[100,], output_size=26,\\\n",
    "                 batch_size=100, lr=0.1, adaptive_lr=False, activation_fn='sigmoid',\\\n",
    "                 max_epoch=100, tol=1e-4, n_iter_no_change=10):\n",
    "        self.input_size=input_size;\n",
    "        self.layers=layers;\n",
    "        self.output_size=output_size;\n",
    "        self.batch_size=batch_size;\n",
    "        self.lr=lr\n",
    "        self.adaptive_lr=adaptive_lr\n",
    "        self.activation_fn=activation_fn\n",
    "        self.max_epoch=max_epoch\n",
    "        self.tol=tol\n",
    "        self.n_iter_no_change=n_iter_no_change\n",
    "        \n",
    "        if activation_fn=='sigmoid':\n",
    "            self.activation_class = sigmoid\n",
    "        elif activation_fn=='relu':\n",
    "            self.activation_class = ReLU\n",
    "        else:\n",
    "            raise Exception(\"Enter valid activation_fn\")\n",
    "        \n",
    "        \n",
    "        self.layers.insert(0, self.input_size); self.layers.append(self.output_size)\n",
    "        self.n_layers = len(self.layers)\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"\"\"MLP(input_size=%s, layers=%s, output_size=%s,\n",
    "    batch_size=%s, lr=%s, adaptive_lr=%s, activation_fn=%s,\n",
    "    tol=%s, n_iter_no_change=%s, max_epoch=%s)\"\"\"%\\\n",
    "                (self.input_size, self.layers[1:-1], self.output_size,\\\n",
    "                self.batch_size, self.lr, self.adaptive_lr, self.activation_fn,\\\n",
    "                self.tol, self.n_iter_no_change, self.max_epoch)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        np.random.seed(0)\n",
    "        f = self.activation_class.f\n",
    "        df = self.activation_class.df\n",
    "        itr=0\n",
    "        \n",
    "        val_indices = np.random.choice(X.shape[0], size=int(X.shape[0]*0.1))\n",
    "        train_indices = np.setdiff1d(list(range(X.shape[0])), val_indices)\n",
    "        X_train, Y_train = X[train_indices], Y[train_indices]\n",
    "        X_val, Y_val = X[val_indices], Y[val_indices]\n",
    "        self.val_loss_lst=[0]\n",
    "        cnt_no_change=0\n",
    "        \n",
    "        if self.activation_fn=='sigmoid':\n",
    "            self._intialize_weights(type='glorot-normal')\n",
    "        else:\n",
    "            self._intialize_weights(type='he-normal')\n",
    "        \n",
    "            \n",
    "        for epoch in tqdm(range(self.max_epoch)):\n",
    "            self.val_loss_lst.append(self._compute_loss(X_val, Y_val))\n",
    "            indices = np.random.choice(X_train.shape[0], size=X_train.shape[0])\n",
    "            X_train_ = X_train[indices]\n",
    "            Y_train_ = Y_train[indices]\n",
    "            if abs(self.val_loss_lst[-1]-self.val_loss_lst[-2])<self.tol:\n",
    "                cnt_no_change+=1\n",
    "                if(cnt_no_change==self.n_iter_no_change):\n",
    "                    print('Converged in %d epochs'%(epoch+1))\n",
    "                    return\n",
    "            else:\n",
    "                cnt_no_change=0\n",
    "            \n",
    "            if self.adaptive_lr:\n",
    "                lr=self.lr/math.sqrt(epoch+1)\n",
    "            else:\n",
    "                lr=self.lr\n",
    "            for i in range(0, X_train.shape[0], self.batch_size):\n",
    "                \n",
    "                X=X_train_[i:i+self.batch_size,:]\n",
    "                Y=Y_train_[i:i+self.batch_size,:]  \n",
    "\n",
    "                # Forward-prop\n",
    "                output = self._forward_prop(X)\n",
    "\n",
    "                # Back-Prop\n",
    "                memo, updates = self._back_prop(output, Y)\n",
    "                \n",
    "                # Update\n",
    "                for i in range(self.n_layers-1):\n",
    "                    self.intercepts[i]=self.intercepts[i] - lr*np.sum(memo[i],axis=0)\n",
    "                    self.weights[i]=self.weights[i] - lr * updates[i]\n",
    "                \n",
    "                itr+=1\n",
    "    \n",
    "    def _forward_prop(self,X):\n",
    "        f = self.activation_class.f\n",
    "        output=[X]\n",
    "        for i in range(self.n_layers-2):\n",
    "            output.append(f(np.dot(output[-1],self.weights[i])+self.intercepts[i]))\n",
    "        output.append(sigmoid.f(np.dot(output[-1],self.weights[-1])+self.intercepts[-1]))\n",
    "        return output\n",
    "    \n",
    "    def _back_prop(self, output, Y):\n",
    "        df=self.activation_class.df\n",
    "        \n",
    "        memo = []\n",
    "        updates = []\n",
    "        curr_memo = (-1/self.batch_size)*(np.multiply(Y-output[-1],sigmoid.df(output[-1])))\n",
    "        memo.insert(0,curr_memo)\n",
    "        curr_update = np.dot(output[-2].T, curr_memo)\n",
    "        updates.insert(0,curr_update)\n",
    "        for i in range(self.n_layers-2, 0, -1):\n",
    "            curr_memo = np.multiply(memo[0].dot(self.weights[i].T),df(output[i]))\n",
    "            memo.insert(0,curr_memo)\n",
    "            curr_update = (output[i-1].T).dot(curr_memo)\n",
    "            updates.insert(0,curr_update)\n",
    "        \n",
    "        return memo, updates\n",
    "    \n",
    "    def _compute_loss(self, X, Y):\n",
    "        output = self._forward_prop(X)\n",
    "        loss = (0.5/X.shape[0])*np.sum((Y-output[-1])**2)\n",
    "        return loss\n",
    "    \n",
    "    def _intialize_weights(self, type='glorot-normal'):\n",
    "        self.weights=[]\n",
    "        self.intercepts=[]\n",
    "        for i in range(self.n_layers-1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i+1]\n",
    "            #Glorot/Xevier Normal initialization\n",
    "            if type=='glorot-normal':\n",
    "                self.weights.append(np.random.normal(scale=(2/(fan_in+fan_out)), size=(fan_in, fan_out)))\n",
    "                self.intercepts.append(np.random.normal(scale=2/(fan_in+fan_out), size=fan_out))\n",
    "            elif type=='he-normal':\n",
    "                self.weights.append(np.random.normal(scale=np.sqrt(2/fan_in), size=(fan_in, fan_out)))\n",
    "                self.intercepts.append(np.random.normal(scale=np.sqrt(2/fan_in), size=fan_out))\n",
    "            elif type='glorot-uniform':\n",
    "                self.weights.append(np.random.uniform(low=-2/(fan_in+fan_out)\\\n",
    "                                                      high=2/(fan_in+fan_out),\\\n",
    "                                                      size=(fan_in, fan_out)))\n",
    "                self.intercepts.append(np.random.uniform(low=-2/(fan_in+fan_out),\\\n",
    "                                                         high=2/(fan_in+fan_out),\\\n",
    "                                                         size=fan_out))\n",
    "            elif type='he-uniform':\n",
    "                self.weights.append(np.random.uniform(low=-np.sqrt(2/fan_in)\\\n",
    "                                                      high=np.sqrt(2/fan_in),\\\n",
    "                                                      size=(fan_in, fan_out)))\n",
    "                self.intercepts.append(np.random.uniform(low=-np.sqrt(2/fan_in),\\\n",
    "                                                         high=np.sqrt(2/fan_in),\\\n",
    "                                                         size=fan_out))\n",
    "                \n",
    "            \n",
    "    def predict(self, X):\n",
    "        pred_proba=self.predict_proba(X)\n",
    "        pred=np.zeros(pred_proba.shape, dtype=np.int8)\n",
    "        pred[np.arange(pred.shape[0]), np.argmax(pred_proba, axis=1)] = 1\n",
    "        return pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        f = self.activation_class.f\n",
    "        \n",
    "        pred_proba=X\n",
    "        for i in range(self.n_layers-2):\n",
    "            pred_proba=f(pred_proba@self.weights[i]+self.intercepts[i])\n",
    "        pred_proba=sigmoid.f(pred_proba@self.weights[-1]+self.intercepts[-1])\n",
    "        \n",
    "        return pred_proba\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        return np.log(self.predict_proba(X))\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        pred = self.predict(X)\n",
    "        y_true = np.argmax(Y, axis=1)\n",
    "        y_pred = np.argmax(pred, axis=1)\n",
    "        return (np.sum(y_true==y_pred)/y_true.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.63741444, -0.48143446,  0.14282974, -0.75221681, -0.28280334],\n",
       "       [-0.60281406,  0.56980761,  0.22617068, -0.32537091, -0.30153951],\n",
       "       [-0.50682585, -0.21341273,  0.02204812, -0.9982623 , -0.88791892]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(scale)\n",
    "np.random.random_(low=-1, high=1, size=(3,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100\n",
    "## 0.9626 0.9056\n",
    "## 1e-7_1_0.9231_0.8833_742"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 98/2000 [00:03<01:02, 30.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 99 epochs\n",
      "0.038461538461538464\n",
      "0.038461538461538464\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[1], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:38<00:00, 26.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10407692307692308\n",
      "0.10046153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=1000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:56<00:00, 26.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1666153846153846\n",
      "0.1716923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=1500,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:15<00:00, 26.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24669230769230768\n",
      "0.24553846153846154\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [01:33<00:00, 26.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32992307692307693\n",
      "0.3150769230769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=2500,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:54<00:00, 26.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36038461538461536\n",
      "0.33892307692307694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=3000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [02:35<00:00, 25.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4543076923076923\n",
      "0.428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=4000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [03:48<00:00, 26.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5954615384615385\n",
      "0.5438461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=6000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:14<00:00, 26.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6620769230769231\n",
      "0.5856923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=10000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [12:22<00:00, 26.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683\n",
      "0.5727692307692308\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=20000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.506\n",
      "0.4887692307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[10], batch_size=100, activation_fn='sigmoid', max_epoch=1000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [01:03<00:00, 23.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683076923076923\n",
      "0.6410769230769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[10], batch_size=100, activation_fn='sigmoid', max_epoch=1500,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:23<00:00, 23.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7201538461538461\n",
      "0.6758461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[10], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [01:47<00:00, 23.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7552307692307693\n",
      "0.7016923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[10], batch_size=100, activation_fn='sigmoid', max_epoch=2500,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [02:05<00:00, 23.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7770769230769231\n",
      "0.7098461538461538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[10], batch_size=100, activation_fn='sigmoid', max_epoch=3000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 16.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9491538461538461\n",
      "0.8849230769230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[50], batch_size=100, activation_fn='sigmoid', max_epoch=100,\\\n",
    "       lr=2, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.962076923076923\n",
      "0.9023076923076923\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[100], batch_size=100, activation_fn='sigmoid', max_epoch=100,\\\n",
    "       lr=2, tol=1e-6, n_iter_no_change=5)\n",
    "ml.fit(X_train, Y_train)\n",
    "print(ml.score(X_train, Y_train))\n",
    "print(ml.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9626153846153847"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9056923076923077"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.040769230769230766, 0.04230769230769231, 0.024615384615384615, 0.03769230769230769, 0.033846153846153845, 0.04230769230769231, 0.04230769230769231]\n"
     ]
    }
   ],
   "source": [
    "print(ml.val_score_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(range(len(ml.val_loss_lst)), ml.val_loss_lst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.040769230769230766,\n",
       " 0.04230769230769231,\n",
       " 0.024615384615384615,\n",
       " 0.03769230769230769,\n",
       " 0.033846153846153845,\n",
       " 0.04230769230769231,\n",
       " 0.04230769230769231,\n",
       " 0.03461538461538462,\n",
       " 0.06846153846153846,\n",
       " 0.04230769230769231,\n",
       " 0.045384615384615384,\n",
       " 0.04230769230769231,\n",
       " 0.04230769230769231,\n",
       " 0.07153846153846154,\n",
       " 0.03461538461538462,\n",
       " 0.03461538461538462,\n",
       " 0.03538461538461538,\n",
       " 0.03461538461538462,\n",
       " 0.03461538461538462,\n",
       " 0.08153846153846153,\n",
       " 0.03461538461538462,\n",
       " 0.03923076923076923,\n",
       " 0.06923076923076923,\n",
       " 0.09769230769230769,\n",
       " 0.10384615384615385,\n",
       " 0.07769230769230769,\n",
       " 0.11538461538461539,\n",
       " 0.05307692307692308,\n",
       " 0.08076923076923077,\n",
       " 0.07384615384615385,\n",
       " 0.1276923076923077,\n",
       " 0.03538461538461538,\n",
       " 0.09769230769230769,\n",
       " 0.06769230769230769,\n",
       " 0.10615384615384615,\n",
       " 0.03923076923076923,\n",
       " 0.17153846153846153,\n",
       " 0.17384615384615384,\n",
       " 0.06615384615384616,\n",
       " 0.12692307692307692,\n",
       " 0.1276923076923077,\n",
       " 0.11,\n",
       " 0.15,\n",
       " 0.1523076923076923,\n",
       " 0.1676923076923077,\n",
       " 0.18307692307692308,\n",
       " 0.2053846153846154,\n",
       " 0.23153846153846153,\n",
       " 0.17384615384615384,\n",
       " 0.1376923076923077,\n",
       " 0.17692307692307693,\n",
       " 0.18076923076923077,\n",
       " 0.19538461538461538,\n",
       " 0.23615384615384616,\n",
       " 0.16153846153846155,\n",
       " 0.21,\n",
       " 0.2246153846153846,\n",
       " 0.18923076923076923,\n",
       " 0.1946153846153846,\n",
       " 0.20615384615384616,\n",
       " 0.16076923076923078,\n",
       " 0.19,\n",
       " 0.19923076923076924,\n",
       " 0.18923076923076923,\n",
       " 0.24846153846153846,\n",
       " 0.25846153846153846,\n",
       " 0.21384615384615385,\n",
       " 0.19692307692307692,\n",
       " 0.21384615384615385,\n",
       " 0.1976923076923077,\n",
       " 0.18076923076923077,\n",
       " 0.22692307692307692,\n",
       " 0.2323076923076923,\n",
       " 0.19692307692307692,\n",
       " 0.23384615384615384,\n",
       " 0.21846153846153846,\n",
       " 0.20923076923076922,\n",
       " 0.24076923076923076,\n",
       " 0.19923076923076924,\n",
       " 0.21384615384615385,\n",
       " 0.21769230769230768,\n",
       " 0.23076923076923078,\n",
       " 0.2553846153846154,\n",
       " 0.2523076923076923,\n",
       " 0.26,\n",
       " 0.2569230769230769,\n",
       " 0.2592307692307692,\n",
       " 0.2676923076923077,\n",
       " 0.26384615384615384,\n",
       " 0.29923076923076924,\n",
       " 0.29846153846153844,\n",
       " 0.3038461538461538,\n",
       " 0.2969230769230769,\n",
       " 0.3046153846153846,\n",
       " 0.32153846153846155,\n",
       " 0.30846153846153845,\n",
       " 0.31846153846153846,\n",
       " 0.33692307692307694,\n",
       " 0.34692307692307695,\n",
       " 0.35,\n",
       " 0.3546153846153846,\n",
       " 0.35384615384615387,\n",
       " 0.3723076923076923,\n",
       " 0.3646153846153846,\n",
       " 0.37538461538461537,\n",
       " 0.39,\n",
       " 0.40384615384615385,\n",
       " 0.4146153846153846,\n",
       " 0.4176923076923077,\n",
       " 0.4307692307692308,\n",
       " 0.4461538461538462,\n",
       " 0.45,\n",
       " 0.4584615384615385,\n",
       " 0.4707692307692308,\n",
       " 0.4684615384615385,\n",
       " 0.4723076923076923,\n",
       " 0.4930769230769231,\n",
       " 0.5007692307692307,\n",
       " 0.5030769230769231,\n",
       " 0.5138461538461538,\n",
       " 0.5169230769230769,\n",
       " 0.5238461538461539,\n",
       " 0.5276923076923077,\n",
       " 0.546923076923077,\n",
       " 0.5484615384615384,\n",
       " 0.5476923076923077,\n",
       " 0.5815384615384616,\n",
       " 0.5823076923076923,\n",
       " 0.5846153846153846,\n",
       " 0.59,\n",
       " 0.6015384615384616,\n",
       " 0.6115384615384616,\n",
       " 0.6269230769230769,\n",
       " 0.6307692307692307,\n",
       " 0.6446153846153846,\n",
       " 0.6438461538461538,\n",
       " 0.6492307692307693,\n",
       " 0.6515384615384615,\n",
       " 0.6607692307692308,\n",
       " 0.6630769230769231,\n",
       " 0.6723076923076923,\n",
       " 0.68,\n",
       " 0.6961538461538461,\n",
       " 0.703076923076923,\n",
       " 0.703076923076923,\n",
       " 0.7061538461538461,\n",
       " 0.713076923076923,\n",
       " 0.7053846153846154,\n",
       " 0.7223076923076923,\n",
       " 0.7269230769230769,\n",
       " 0.7261538461538461,\n",
       " 0.7276923076923076,\n",
       " 0.7323076923076923,\n",
       " 0.7392307692307692,\n",
       " 0.7369230769230769,\n",
       " 0.7392307692307692,\n",
       " 0.7407692307692307,\n",
       " 0.7469230769230769,\n",
       " 0.75,\n",
       " 0.7576923076923077,\n",
       " 0.7553846153846154,\n",
       " 0.7607692307692308,\n",
       " 0.7607692307692308,\n",
       " 0.7607692307692308,\n",
       " 0.7692307692307693,\n",
       " 0.7676923076923077,\n",
       " 0.7684615384615384,\n",
       " 0.7723076923076924,\n",
       " 0.7707692307692308,\n",
       " 0.7746153846153846,\n",
       " 0.7761538461538462,\n",
       " 0.7753846153846153,\n",
       " 0.78,\n",
       " 0.7784615384615384,\n",
       " 0.7830769230769231,\n",
       " 0.7830769230769231,\n",
       " 0.7861538461538462,\n",
       " 0.7846153846153846,\n",
       " 0.786923076923077,\n",
       " 0.7876923076923077,\n",
       " 0.7946153846153846,\n",
       " 0.7907692307692308,\n",
       " 0.7930769230769231,\n",
       " 0.7946153846153846,\n",
       " 0.7953846153846154,\n",
       " 0.7961538461538461,\n",
       " 0.8015384615384615,\n",
       " 0.7976923076923077,\n",
       " 0.8,\n",
       " 0.8015384615384615,\n",
       " 0.8038461538461539,\n",
       " 0.8015384615384615,\n",
       " 0.8046153846153846,\n",
       " 0.8046153846153846,\n",
       " 0.8053846153846154,\n",
       " 0.8061538461538461,\n",
       " 0.8046153846153846,\n",
       " 0.8061538461538461,\n",
       " 0.8092307692307692,\n",
       " 0.806923076923077,\n",
       " 0.8084615384615385,\n",
       " 0.8053846153846154,\n",
       " 0.8061538461538461,\n",
       " 0.8038461538461539,\n",
       " 0.806923076923077,\n",
       " 0.806923076923077,\n",
       " 0.8107692307692308,\n",
       " 0.8092307692307692,\n",
       " 0.8107692307692308,\n",
       " 0.8115384615384615,\n",
       " 0.8107692307692308,\n",
       " 0.8123076923076923,\n",
       " 0.8153846153846154,\n",
       " 0.8115384615384615,\n",
       " 0.813076923076923,\n",
       " 0.8115384615384615,\n",
       " 0.813076923076923,\n",
       " 0.8146153846153846,\n",
       " 0.8161538461538461,\n",
       " 0.8207692307692308,\n",
       " 0.816923076923077,\n",
       " 0.8176923076923077,\n",
       " 0.8161538461538461,\n",
       " 0.8153846153846154,\n",
       " 0.8184615384615385,\n",
       " 0.8192307692307692,\n",
       " 0.8161538461538461,\n",
       " 0.82,\n",
       " 0.82,\n",
       " 0.8146153846153846,\n",
       " 0.816923076923077,\n",
       " 0.8192307692307692,\n",
       " 0.82,\n",
       " 0.82,\n",
       " 0.8207692307692308,\n",
       " 0.8238461538461539,\n",
       " 0.823076923076923,\n",
       " 0.8246153846153846,\n",
       " 0.8223076923076923,\n",
       " 0.8246153846153846,\n",
       " 0.8261538461538461,\n",
       " 0.8238461538461539,\n",
       " 0.8261538461538461,\n",
       " 0.8253846153846154,\n",
       " 0.8284615384615385,\n",
       " 0.8269230769230769,\n",
       " 0.8269230769230769,\n",
       " 0.8269230769230769,\n",
       " 0.8276923076923077,\n",
       " 0.8269230769230769,\n",
       " 0.8276923076923077,\n",
       " 0.8276923076923077,\n",
       " 0.8315384615384616,\n",
       " 0.83,\n",
       " 0.8292307692307692,\n",
       " 0.8315384615384616,\n",
       " 0.8315384615384616,\n",
       " 0.83,\n",
       " 0.8315384615384616,\n",
       " 0.8284615384615385,\n",
       " 0.83,\n",
       " 0.8284615384615385,\n",
       " 0.83,\n",
       " 0.8307692307692308,\n",
       " 0.8307692307692308,\n",
       " 0.833076923076923,\n",
       " 0.8338461538461538,\n",
       " 0.8323076923076923,\n",
       " 0.8323076923076923,\n",
       " 0.8353846153846154,\n",
       " 0.8361538461538461,\n",
       " 0.833076923076923,\n",
       " 0.8346153846153846,\n",
       " 0.8353846153846154,\n",
       " 0.8376923076923077,\n",
       " 0.8361538461538461,\n",
       " 0.8384615384615385,\n",
       " 0.8407692307692308,\n",
       " 0.8392307692307692,\n",
       " 0.8384615384615385,\n",
       " 0.84,\n",
       " 0.84,\n",
       " 0.8423076923076923,\n",
       " 0.8423076923076923,\n",
       " 0.8430769230769231,\n",
       " 0.8415384615384616,\n",
       " 0.8430769230769231,\n",
       " 0.8446153846153847,\n",
       " 0.8430769230769231,\n",
       " 0.8438461538461538,\n",
       " 0.8461538461538461,\n",
       " 0.8446153846153847,\n",
       " 0.8438461538461538,\n",
       " 0.8446153846153847,\n",
       " 0.8446153846153847,\n",
       " 0.8423076923076923,\n",
       " 0.8453846153846154,\n",
       " 0.8476923076923077,\n",
       " 0.8469230769230769,\n",
       " 0.8484615384615385,\n",
       " 0.8469230769230769,\n",
       " 0.85,\n",
       " 0.8507692307692307,\n",
       " 0.8507692307692307,\n",
       " 0.8515384615384616,\n",
       " 0.8507692307692307,\n",
       " 0.8530769230769231,\n",
       " 0.8492307692307692,\n",
       " 0.8523076923076923,\n",
       " 0.8546153846153847,\n",
       " 0.8523076923076923,\n",
       " 0.8530769230769231,\n",
       " 0.8515384615384616,\n",
       " 0.8538461538461538,\n",
       " 0.8538461538461538,\n",
       " 0.8515384615384616,\n",
       " 0.8530769230769231,\n",
       " 0.8553846153846154,\n",
       " 0.8569230769230769,\n",
       " 0.8553846153846154,\n",
       " 0.8553846153846154,\n",
       " 0.8553846153846154,\n",
       " 0.8530769230769231,\n",
       " 0.8530769230769231,\n",
       " 0.8507692307692307,\n",
       " 0.8530769230769231,\n",
       " 0.8546153846153847,\n",
       " 0.8530769230769231,\n",
       " 0.8530769230769231,\n",
       " 0.8515384615384616,\n",
       " 0.8515384615384616,\n",
       " 0.8523076923076923,\n",
       " 0.8538461538461538,\n",
       " 0.8530769230769231,\n",
       " 0.8530769230769231,\n",
       " 0.8561538461538462,\n",
       " 0.8561538461538462,\n",
       " 0.8538461538461538,\n",
       " 0.8538461538461538,\n",
       " 0.8546153846153847,\n",
       " 0.8569230769230769,\n",
       " 0.8546153846153847,\n",
       " 0.8530769230769231,\n",
       " 0.8584615384615385,\n",
       " 0.8569230769230769,\n",
       " 0.8561538461538462,\n",
       " 0.8553846153846154,\n",
       " 0.8561538461538462,\n",
       " 0.8592307692307692,\n",
       " 0.8561538461538462,\n",
       " 0.8584615384615385,\n",
       " 0.8576923076923076,\n",
       " 0.8576923076923076,\n",
       " 0.8569230769230769,\n",
       " 0.8561538461538462,\n",
       " 0.8561538461538462,\n",
       " 0.8592307692307692,\n",
       " 0.8576923076923076,\n",
       " 0.8576923076923076,\n",
       " 0.86,\n",
       " 0.8584615384615385,\n",
       " 0.8592307692307692,\n",
       " 0.8584615384615385,\n",
       " 0.86,\n",
       " 0.8576923076923076,\n",
       " 0.86,\n",
       " 0.8615384615384616,\n",
       " 0.86,\n",
       " 0.86,\n",
       " 0.8592307692307692,\n",
       " 0.8592307692307692,\n",
       " 0.8615384615384616,\n",
       " 0.8638461538461538,\n",
       " 0.86,\n",
       " 0.8615384615384616,\n",
       " 0.8607692307692307,\n",
       " 0.8607692307692307,\n",
       " 0.8623076923076923,\n",
       " 0.8623076923076923,\n",
       " 0.8630769230769231,\n",
       " 0.8615384615384616,\n",
       " 0.8615384615384616,\n",
       " 0.8615384615384616,\n",
       " 0.8623076923076923,\n",
       " 0.86,\n",
       " 0.8607692307692307,\n",
       " 0.8661538461538462,\n",
       " 0.8646153846153846,\n",
       " 0.86,\n",
       " 0.8630769230769231,\n",
       " 0.8630769230769231,\n",
       " 0.8638461538461538,\n",
       " 0.8646153846153846,\n",
       " 0.8653846153846154,\n",
       " 0.8653846153846154,\n",
       " 0.8646153846153846,\n",
       " 0.8669230769230769,\n",
       " 0.8646153846153846,\n",
       " 0.8638461538461538,\n",
       " 0.8653846153846154,\n",
       " 0.8646153846153846,\n",
       " 0.8646153846153846,\n",
       " 0.8630769230769231,\n",
       " 0.8653846153846154,\n",
       " 0.8638461538461538,\n",
       " 0.8653846153846154,\n",
       " 0.8684615384615385,\n",
       " 0.8653846153846154,\n",
       " 0.8653846153846154,\n",
       " 0.8653846153846154,\n",
       " 0.8661538461538462,\n",
       " 0.8676923076923077,\n",
       " 0.8661538461538462,\n",
       " 0.8646153846153846,\n",
       " 0.8630769230769231,\n",
       " 0.8692307692307693,\n",
       " 0.8684615384615385,\n",
       " 0.8653846153846154,\n",
       " 0.8684615384615385,\n",
       " 0.8707692307692307,\n",
       " 0.8676923076923077,\n",
       " 0.8684615384615385,\n",
       " 0.8692307692307693,\n",
       " 0.8669230769230769,\n",
       " 0.87,\n",
       " 0.87,\n",
       " 0.8723076923076923,\n",
       " 0.8723076923076923,\n",
       " 0.8730769230769231,\n",
       " 0.87,\n",
       " 0.8715384615384615,\n",
       " 0.8715384615384615,\n",
       " 0.8692307692307693,\n",
       " 0.8707692307692307,\n",
       " 0.87,\n",
       " 0.87,\n",
       " 0.8707692307692307,\n",
       " 0.8723076923076923,\n",
       " 0.8684615384615385,\n",
       " 0.87,\n",
       " 0.8707692307692307,\n",
       " 0.87,\n",
       " 0.87,\n",
       " 0.87,\n",
       " 0.87,\n",
       " 0.8723076923076923,\n",
       " 0.8738461538461538,\n",
       " 0.8730769230769231,\n",
       " 0.8738461538461538,\n",
       " 0.8715384615384615,\n",
       " 0.8707692307692307,\n",
       " 0.8723076923076923,\n",
       " 0.8730769230769231,\n",
       " 0.8761538461538462,\n",
       " 0.8715384615384615,\n",
       " 0.8769230769230769,\n",
       " 0.8715384615384615,\n",
       " 0.8753846153846154,\n",
       " 0.8746153846153846,\n",
       " 0.8746153846153846,\n",
       " 0.8738461538461538,\n",
       " 0.8753846153846154,\n",
       " 0.8738461538461538,\n",
       " 0.8723076923076923,\n",
       " 0.8761538461538462,\n",
       " 0.8746153846153846,\n",
       " 0.8769230769230769,\n",
       " 0.8723076923076923,\n",
       " 0.8746153846153846,\n",
       " 0.8738461538461538,\n",
       " 0.8746153846153846,\n",
       " 0.8753846153846154,\n",
       " 0.8746153846153846,\n",
       " 0.8746153846153846,\n",
       " 0.8761538461538462,\n",
       " 0.8753846153846154,\n",
       " 0.8761538461538462,\n",
       " 0.8769230769230769,\n",
       " 0.8738461538461538,\n",
       " 0.8761538461538462,\n",
       " 0.8746153846153846,\n",
       " 0.8761538461538462,\n",
       " 0.8792307692307693,\n",
       " 0.8807692307692307,\n",
       " 0.8746153846153846,\n",
       " 0.8753846153846154,\n",
       " 0.8723076923076923,\n",
       " 0.8761538461538462,\n",
       " 0.8769230769230769,\n",
       " 0.8792307692307693,\n",
       " 0.88,\n",
       " 0.8776923076923077,\n",
       " 0.8769230769230769,\n",
       " 0.88,\n",
       " 0.8792307692307693,\n",
       " 0.88,\n",
       " 0.8807692307692307,\n",
       " 0.88,\n",
       " 0.8776923076923077,\n",
       " 0.8761538461538462,\n",
       " 0.8761538461538462,\n",
       " 0.8761538461538462,\n",
       " 0.8746153846153846,\n",
       " 0.8769230769230769,\n",
       " 0.88,\n",
       " 0.8776923076923077,\n",
       " 0.8761538461538462,\n",
       " 0.8792307692307693,\n",
       " 0.88,\n",
       " 0.8776923076923077,\n",
       " 0.8761538461538462,\n",
       " 0.8769230769230769,\n",
       " 0.8792307692307693,\n",
       " 0.8761538461538462,\n",
       " 0.88,\n",
       " 0.8807692307692307,\n",
       " 0.8823076923076923,\n",
       " 0.8815384615384615,\n",
       " 0.8830769230769231,\n",
       " 0.8815384615384615,\n",
       " 0.8807692307692307,\n",
       " 0.8815384615384615,\n",
       " 0.8792307692307693,\n",
       " 0.8815384615384615,\n",
       " 0.8815384615384615,\n",
       " 0.8815384615384615,\n",
       " 0.8815384615384615,\n",
       " 0.8807692307692307,\n",
       " 0.8807692307692307,\n",
       " 0.8792307692307693,\n",
       " 0.8830769230769231,\n",
       " 0.88,\n",
       " 0.8815384615384615,\n",
       " 0.8807692307692307,\n",
       " 0.8792307692307693,\n",
       " 0.8823076923076923,\n",
       " 0.8838461538461538,\n",
       " 0.8838461538461538,\n",
       " 0.8823076923076923,\n",
       " 0.8830769230769231,\n",
       " 0.8823076923076923,\n",
       " 0.8815384615384615,\n",
       " 0.8830769230769231,\n",
       " 0.8838461538461538,\n",
       " 0.8846153846153846,\n",
       " 0.8823076923076923,\n",
       " 0.8846153846153846,\n",
       " 0.8838461538461538,\n",
       " 0.8861538461538462,\n",
       " 0.8838461538461538,\n",
       " 0.8846153846153846,\n",
       " 0.8853846153846154,\n",
       " 0.8869230769230769,\n",
       " 0.8838461538461538,\n",
       " 0.8846153846153846,\n",
       " 0.8853846153846154,\n",
       " 0.8846153846153846,\n",
       " 0.8869230769230769,\n",
       " 0.8838461538461538,\n",
       " 0.8861538461538462,\n",
       " 0.8838461538461538,\n",
       " 0.8861538461538462,\n",
       " 0.8853846153846154,\n",
       " 0.8884615384615384,\n",
       " 0.8869230769230769,\n",
       " 0.8838461538461538,\n",
       " 0.8853846153846154,\n",
       " 0.8861538461538462,\n",
       " 0.8861538461538462,\n",
       " 0.8861538461538462,\n",
       " 0.8876923076923077,\n",
       " 0.8861538461538462,\n",
       " 0.8884615384615384,\n",
       " 0.8861538461538462,\n",
       " 0.8876923076923077,\n",
       " 0.8861538461538462,\n",
       " 0.8876923076923077,\n",
       " 0.8876923076923077,\n",
       " 0.8853846153846154,\n",
       " 0.8876923076923077,\n",
       " 0.8884615384615384,\n",
       " 0.8876923076923077,\n",
       " 0.8853846153846154,\n",
       " 0.8869230769230769,\n",
       " 0.8869230769230769,\n",
       " 0.89,\n",
       " 0.8869230769230769,\n",
       " 0.8884615384615384,\n",
       " 0.8876923076923077,\n",
       " 0.8876923076923077,\n",
       " 0.8892307692307693,\n",
       " 0.8869230769230769,\n",
       " 0.8892307692307693,\n",
       " 0.8876923076923077,\n",
       " 0.8869230769230769,\n",
       " 0.8884615384615384,\n",
       " 0.8876923076923077,\n",
       " 0.8907692307692308,\n",
       " 0.8884615384615384,\n",
       " 0.89,\n",
       " 0.8876923076923077,\n",
       " 0.8892307692307693,\n",
       " 0.89,\n",
       " 0.8892307692307693,\n",
       " 0.8907692307692308,\n",
       " 0.8915384615384615,\n",
       " 0.8907692307692308,\n",
       " 0.89,\n",
       " 0.8892307692307693,\n",
       " 0.89,\n",
       " 0.8884615384615384,\n",
       " 0.8892307692307693,\n",
       " 0.8907692307692308,\n",
       " 0.8907692307692308,\n",
       " 0.89,\n",
       " 0.8907692307692308,\n",
       " 0.89,\n",
       " 0.8907692307692308,\n",
       " 0.8907692307692308,\n",
       " 0.89,\n",
       " 0.89,\n",
       " 0.89,\n",
       " 0.89,\n",
       " 0.8923076923076924,\n",
       " 0.8915384615384615,\n",
       " 0.8923076923076924,\n",
       " 0.8915384615384615,\n",
       " 0.8907692307692308,\n",
       " 0.8923076923076924,\n",
       " 0.8915384615384615,\n",
       " 0.8915384615384615,\n",
       " 0.8907692307692308,\n",
       " 0.8923076923076924,\n",
       " 0.8923076923076924,\n",
       " 0.8923076923076924,\n",
       " 0.8915384615384615,\n",
       " 0.8907692307692308,\n",
       " 0.8892307692307693,\n",
       " 0.8907692307692308,\n",
       " 0.8915384615384615,\n",
       " 0.89,\n",
       " 0.8915384615384615,\n",
       " 0.8915384615384615,\n",
       " 0.8923076923076924,\n",
       " 0.8907692307692308,\n",
       " 0.8923076923076924,\n",
       " 0.8907692307692308,\n",
       " 0.8907692307692308,\n",
       " 0.8907692307692308,\n",
       " 0.8915384615384615,\n",
       " 0.8907692307692308,\n",
       " 0.8907692307692308,\n",
       " 0.8907692307692308,\n",
       " 0.8907692307692308,\n",
       " 0.8884615384615384,\n",
       " 0.89,\n",
       " 0.89,\n",
       " 0.8961538461538462,\n",
       " 0.8930769230769231,\n",
       " 0.8961538461538462,\n",
       " 0.8923076923076924,\n",
       " 0.8938461538461538,\n",
       " 0.8923076923076924,\n",
       " 0.8915384615384615,\n",
       " 0.8915384615384615,\n",
       " 0.8938461538461538,\n",
       " 0.8915384615384615,\n",
       " 0.8915384615384615,\n",
       " 0.8930769230769231,\n",
       " 0.8892307692307693,\n",
       " 0.8892307692307693,\n",
       " 0.8930769230769231,\n",
       " 0.8930769230769231,\n",
       " 0.8930769230769231,\n",
       " 0.8946153846153846,\n",
       " 0.8946153846153846,\n",
       " 0.8953846153846153,\n",
       " 0.8938461538461538,\n",
       " 0.8946153846153846,\n",
       " 0.8938461538461538,\n",
       " 0.8923076923076924,\n",
       " 0.8946153846153846,\n",
       " 0.8953846153846153,\n",
       " 0.8923076923076924,\n",
       " 0.8946153846153846,\n",
       " 0.8938461538461538,\n",
       " 0.8946153846153846,\n",
       " 0.8953846153846153,\n",
       " 0.8961538461538462,\n",
       " 0.8946153846153846,\n",
       " 0.8938461538461538,\n",
       " 0.8953846153846153,\n",
       " 0.8953846153846153,\n",
       " 0.8946153846153846,\n",
       " 0.8930769230769231,\n",
       " 0.8915384615384615,\n",
       " 0.8930769230769231,\n",
       " 0.8923076923076924,\n",
       " 0.8930769230769231,\n",
       " 0.8923076923076924,\n",
       " 0.8938461538461538,\n",
       " 0.8961538461538462,\n",
       " 0.8946153846153846,\n",
       " 0.8938461538461538,\n",
       " 0.8938461538461538,\n",
       " 0.8953846153846153,\n",
       " 0.8946153846153846,\n",
       " 0.8915384615384615,\n",
       " 0.8907692307692308,\n",
       " 0.8923076923076924,\n",
       " 0.8946153846153846,\n",
       " 0.8938461538461538,\n",
       " 0.8930769230769231,\n",
       " 0.8915384615384615,\n",
       " 0.8946153846153846,\n",
       " 0.8938461538461538,\n",
       " 0.8953846153846153,\n",
       " 0.8930769230769231,\n",
       " 0.8969230769230769,\n",
       " 0.8984615384615384,\n",
       " 0.8969230769230769,\n",
       " 0.8946153846153846,\n",
       " 0.8946153846153846,\n",
       " 0.8946153846153846,\n",
       " 0.8961538461538462,\n",
       " 0.8938461538461538,\n",
       " 0.8938461538461538,\n",
       " 0.8953846153846153,\n",
       " 0.8961538461538462,\n",
       " 0.8938461538461538,\n",
       " 0.8961538461538462,\n",
       " 0.8961538461538462,\n",
       " 0.8938461538461538,\n",
       " 0.8946153846153846,\n",
       " 0.8923076923076924,\n",
       " 0.8984615384615384,\n",
       " 0.8969230769230769,\n",
       " 0.8938461538461538,\n",
       " 0.8930769230769231,\n",
       " 0.8915384615384615,\n",
       " 0.8961538461538462,\n",
       " 0.8938461538461538,\n",
       " 0.8969230769230769,\n",
       " 0.8946153846153846,\n",
       " 0.8930769230769231,\n",
       " 0.8923076923076924,\n",
       " 0.8923076923076924,\n",
       " 0.8969230769230769,\n",
       " 0.8992307692307693,\n",
       " 0.8930769230769231,\n",
       " 0.8930769230769231,\n",
       " 0.8915384615384615,\n",
       " 0.8938461538461538,\n",
       " 0.8961538461538462,\n",
       " 0.8953846153846153,\n",
       " 0.8953846153846153,\n",
       " 0.8976923076923077,\n",
       " 0.8946153846153846,\n",
       " 0.8969230769230769,\n",
       " 0.8976923076923077,\n",
       " 0.8961538461538462,\n",
       " 0.8976923076923077,\n",
       " 0.8969230769230769,\n",
       " 0.8976923076923077,\n",
       " 0.8984615384615384,\n",
       " 0.8976923076923077,\n",
       " 0.8946153846153846,\n",
       " 0.8969230769230769,\n",
       " 0.8953846153846153,\n",
       " 0.8961538461538462,\n",
       " 0.8969230769230769,\n",
       " 0.8953846153846153,\n",
       " 0.8976923076923077,\n",
       " 0.8969230769230769,\n",
       " 0.8961538461538462,\n",
       " 0.8961538461538462,\n",
       " 0.8938461538461538,\n",
       " 0.8984615384615384,\n",
       " 0.8961538461538462,\n",
       " 0.8992307692307693,\n",
       " 0.9,\n",
       " 0.8984615384615384,\n",
       " 0.8969230769230769,\n",
       " 0.8969230769230769,\n",
       " 0.8984615384615384,\n",
       " 0.8976923076923077,\n",
       " 0.8961538461538462,\n",
       " 0.8969230769230769,\n",
       " 0.8984615384615384,\n",
       " 0.8946153846153846,\n",
       " 0.8961538461538462,\n",
       " 0.8976923076923077,\n",
       " 0.8961538461538462,\n",
       " 0.8953846153846153,\n",
       " 0.8953846153846153,\n",
       " 0.8961538461538462,\n",
       " 0.8992307692307693,\n",
       " 0.8961538461538462,\n",
       " 0.8976923076923077,\n",
       " 0.8984615384615384,\n",
       " 0.8938461538461538,\n",
       " 0.8976923076923077,\n",
       " 0.8992307692307693,\n",
       " 0.8969230769230769,\n",
       " 0.8953846153846153,\n",
       " 0.8961538461538462,\n",
       " 0.8976923076923077,\n",
       " 0.8984615384615384,\n",
       " 0.8953846153846153,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8953846153846153,\n",
       " 0.8976923076923077,\n",
       " 0.8969230769230769,\n",
       " 0.8976923076923077,\n",
       " 0.8992307692307693,\n",
       " 0.8992307692307693,\n",
       " 0.8961538461538462,\n",
       " 0.8961538461538462,\n",
       " 0.8976923076923077,\n",
       " 0.8969230769230769,\n",
       " 0.8976923076923077,\n",
       " 0.8961538461538462,\n",
       " 0.8953846153846153,\n",
       " 0.8969230769230769,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8992307692307693,\n",
       " 0.8992307692307693,\n",
       " 0.8992307692307693,\n",
       " 0.8961538461538462,\n",
       " 0.8953846153846153,\n",
       " 0.8984615384615384,\n",
       " 0.8969230769230769,\n",
       " 0.8961538461538462,\n",
       " 0.8992307692307693,\n",
       " 0.8961538461538462,\n",
       " 0.8969230769230769,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8992307692307693,\n",
       " 0.9,\n",
       " 0.8992307692307693,\n",
       " 0.9,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8976923076923077,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8969230769230769,\n",
       " 0.8961538461538462,\n",
       " 0.8969230769230769,\n",
       " 0.8984615384615384,\n",
       " 0.8992307692307693,\n",
       " 0.8992307692307693,\n",
       " 0.9015384615384615,\n",
       " 0.8992307692307693,\n",
       " 0.9007692307692308,\n",
       " 0.8992307692307693,\n",
       " 0.8984615384615384,\n",
       " 0.8976923076923077,\n",
       " 0.8976923076923077,\n",
       " 0.8976923076923077,\n",
       " 0.9,\n",
       " 0.8976923076923077,\n",
       " 0.8992307692307693,\n",
       " 0.8961538461538462,\n",
       " 0.9007692307692308,\n",
       " 0.9,\n",
       " 0.8984615384615384,\n",
       " 0.8961538461538462,\n",
       " 0.9,\n",
       " 0.8969230769230769,\n",
       " 0.8992307692307693,\n",
       " 0.9007692307692308,\n",
       " 0.9023076923076923,\n",
       " 0.9023076923076923,\n",
       " 0.8992307692307693,\n",
       " 0.8984615384615384,\n",
       " 0.9,\n",
       " 0.8976923076923077,\n",
       " 0.8976923076923077,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.8976923076923077,\n",
       " 0.8976923076923077,\n",
       " 0.8992307692307693,\n",
       " 0.8984615384615384,\n",
       " 0.8976923076923077,\n",
       " 0.8976923076923077,\n",
       " 0.8976923076923077,\n",
       " 0.9,\n",
       " 0.8984615384615384,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9,\n",
       " 0.8992307692307693,\n",
       " 0.8976923076923077,\n",
       " 0.8976923076923077,\n",
       " 0.8992307692307693,\n",
       " 0.8984615384615384,\n",
       " 0.8992307692307693,\n",
       " 0.9007692307692308,\n",
       " 0.8984615384615384,\n",
       " 0.9007692307692308,\n",
       " 0.9,\n",
       " 0.8992307692307693,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8992307692307693,\n",
       " 0.9,\n",
       " 0.9007692307692308,\n",
       " 0.8984615384615384,\n",
       " 0.8984615384615384,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9007692307692308,\n",
       " 0.8992307692307693,\n",
       " 0.9007692307692308,\n",
       " 0.9007692307692308,\n",
       " 0.8984615384615384,\n",
       " 0.8976923076923077,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9015384615384615,\n",
       " 0.9023076923076923,\n",
       " 0.9015384615384615,\n",
       " 0.9007692307692308,\n",
       " 0.9023076923076923,\n",
       " 0.9015384615384615,\n",
       " 0.9023076923076923,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9023076923076923,\n",
       " 0.9007692307692308,\n",
       " 0.9015384615384615,\n",
       " 0.9023076923076923,\n",
       " 0.9007692307692308,\n",
       " 0.9,\n",
       " 0.9023076923076923,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9007692307692308,\n",
       " 0.9023076923076923,\n",
       " 0.9023076923076923,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9007692307692308,\n",
       " 0.9023076923076923,\n",
       " 0.8992307692307693,\n",
       " 0.9015384615384615,\n",
       " 0.9007692307692308,\n",
       " 0.9015384615384615,\n",
       " 0.9023076923076923,\n",
       " 0.9023076923076923,\n",
       " 0.9,\n",
       " 0.9023076923076923,\n",
       " 0.9007692307692308,\n",
       " 0.9015384615384615,\n",
       " 0.8992307692307693,\n",
       " 0.9023076923076923,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9007692307692308,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9030769230769231,\n",
       " 0.9007692307692308,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9030769230769231,\n",
       " 0.9023076923076923,\n",
       " 0.9023076923076923,\n",
       " 0.9023076923076923,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9007692307692308,\n",
       " 0.9015384615384615,\n",
       " 0.9007692307692308,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " 0.9023076923076923,\n",
       " 0.9015384615384615,\n",
       " 0.9015384615384615,\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.val_score_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Training MLP model with 1 neurons in hidden layer:\n",
      "Trained in 3.28 minutes\n",
      "Test Accuracy : 3.85 %\n",
      "Test Accuracy : 3.85 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 5 neurons in hidden layer:\n",
      "Trained in 3.41 minutes\n",
      "Test Accuracy : 24.14 %\n",
      "Test Accuracy : 24.49 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 10 neurons in hidden layer:\n",
      "Trained in 3.52 minutes\n",
      "Test Accuracy : 65.25 %\n",
      "Test Accuracy : 68.62 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 50 neurons in hidden layer:\n",
      "Trained in 4.41 minutes\n",
      "Test Accuracy : 89.12 %\n",
      "Test Accuracy : 95.60 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 100 neurons in hidden layer:\n",
      "Trained in 5.41 minutes\n",
      "Test Accuracy : 90.66 %\n",
      "Test Accuracy : 96.85 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_neurons_lst = [1,5,10,50,100]\n",
    "for n_neurons in n_neurons_lst:\n",
    "    print('*'*50)\n",
    "    print('Training MLP model with %d neurons in hidden layer:'%(n_neurons))\n",
    "    ml=MLP(layers=[n_neurons], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1)\n",
    "    t0=time()\n",
    "    ml.fit(X_train, Y_train)\n",
    "    print('Trained in %.2f minutes'%((time()-t0)/60))\n",
    "    print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test)*100,'%'))\n",
    "    print('Train Accuracy : %.2f %c'%(ml.score(X_train, Y_train)*100,'%'))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Training MLP model with 1 neurons in hidden layer:\n",
      "Trained in 3.20 minutes\n",
      "Test Accuracy : 3.85 %\n",
      "Test Accuracy : 3.85 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 5 neurons in hidden layer:\n",
      "Trained in 3.30 minutes\n",
      "Test Accuracy : 4.17 %\n",
      "Test Accuracy : 4.04 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 10 neurons in hidden layer:\n",
      "Trained in 3.50 minutes\n",
      "Test Accuracy : 7.97 %\n",
      "Test Accuracy : 7.80 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 50 neurons in hidden layer:\n",
      "Trained in 4.46 minutes\n",
      "Test Accuracy : 85.37 %\n",
      "Test Accuracy : 88.85 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 100 neurons in hidden layer:\n",
      "Trained in 5.51 minutes\n",
      "Test Accuracy : 86.35 %\n",
      "Test Accuracy : 89.85 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_neurons_lst = [1,5,10,50,100]\n",
    "for n_neurons in n_neurons_lst:\n",
    "    print('*'*50)\n",
    "    print('Training MLP model with %d neurons in hidden layer:'%(n_neurons))\n",
    "    ml=MLP(layers=[n_neurons], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.5, adaptive_lr=True)\n",
    "    t0=time()\n",
    "    ml.fit(X_train, Y_train)\n",
    "    print('Trained in %.2f minutes'%((time()-t0)/60))\n",
    "    print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "    print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained in 9.58 minutes\n",
      "Test Accuracy : 84.83 %\n",
      "Test Accuracy : 90.24 %\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[100,100], batch_size=100, activation_fn='relu', max_epoch=3000,\\\n",
    "       lr=0.5, adaptive_lr=True)\n",
    "t0=time()\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Trained in %.2f minutes'%((time()-t0)/60))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained in 6.53 minutes\n",
      "Test Accuracy : 3.85 %\n",
      "Test Accuracy : 3.85 %\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[100,100], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.5, adaptive_lr=True)\n",
    "t0=time()\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Trained in %.2f minutes'%((time()-t0)/60))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Training MLP model with 1 neurons in hidden layer:\n",
      "Trained in 2.38 minutes\n",
      "Test Accuracy : 3.85 %\n",
      "Test Accuracy : 3.85 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 5 neurons in hidden layer:\n",
      "Trained in 2.49 minutes\n",
      "Test Accuracy : 14.65 %\n",
      "Test Accuracy : 14.98 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 10 neurons in hidden layer:\n",
      "Trained in 2.53 minutes\n",
      "Test Accuracy : 46.58 %\n",
      "Test Accuracy : 48.54 %\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Training MLP model with 50 neurons in hidden layer:\n"
     ]
    }
   ],
   "source": [
    "n_neurons_lst = [1,5,10,50,100]\n",
    "for n_neurons in n_neurons_lst:\n",
    "    print('*'*50)\n",
    "    print('Training MLP model with %d neurons in hidden layer:'%(n_neurons))\n",
    "    ml=MLP(layers=[n_neurons], batch_size=100, activation_fn='sigmoid', max_epoch=1500,\\\n",
    "       lr=0.1)\n",
    "    t0=time()\n",
    "    ml.fit(X_train, Y_train)\n",
    "    print('Trained in %.2f minutes'%((time()-t0)/60))\n",
    "    print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "    print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 127/2000 [00:20<05:06,  6.11it/s]"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[100], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-5, n_iter_no_change=2, n_iter_avg=1000)\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 960/2000 [02:48<03:02,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 960 epochs\n",
      "Test Accuracy : 89.11 %\n",
      "Test Accuracy : 94.11 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[100], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-3, n_iter_no_change=3, n_iter_avg=1000)\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:22<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 89.12 %\n",
      "Test Accuracy : 95.60 %\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[50], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-3, n_iter_no_change=3, n_iter_avg=1000)\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:03<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 69.40 %\n",
      "Test Accuracy : 74.74 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[10], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-7, n_iter_no_change=2, n_iter_avg=100)\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 80/2000 [00:08<03:12, 10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 80 epochs\n",
      "Test Accuracy : 3.85 %\n",
      "Test Accuracy : 3.85 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[10], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-3, n_iter_no_change=3, n_iter_avg=100)\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:59<00:00,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 71.72 %\n",
      "Test Accuracy : 77.61 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ml=MLP(layers=[10], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=5, n_iter_avg=100)\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17900800009948717"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.loss_lst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(range(len(ml.loss_lst)), ml.loss_lst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 620/2000 [01:02<02:18,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 620 epochs\n",
      "Test Accuracy : 6.03 %\n",
      "Test Accuracy : 6.38 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[5], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-3, n_iter_no_change=30, n_iter_avg=1000)\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:17<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 3.85 %\n",
      "Test Accuracy : 3.85 %\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[1], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-40, n_iter_no_change=2, n_iter_avg=1000)\n",
    "ml.fit(X_train, Y_train)\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_test, Y_test),'%'))\n",
    "print('Test Accuracy : %.2f %c'%(ml.score(X_train, Y_train),'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:24<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.64615384615385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[50], batch_size=100, activation_fn='sigmoid', max_epoch=2000,\\\n",
    "       lr=0.1, tol=1e-6, n_iter_no_change=1)\n",
    "ml.fit(X_train, Y_train)\n",
    "ml.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1092/2000 [02:58<02:28,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 1092 epoches\n",
      "89.92307692307692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.3\n"
     ]
    }
   ],
   "source": [
    "ml.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(input_size=784, layers=[50], output_size=26,\n",
       "    batch_size=100, lr=0.1, adaptive_lr=False, activation_fn=sigmoid,\n",
       "    tol=0.0001, n_iter_no_change=10, max_epoch=2000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "check_after_iter = 100\n",
    "for i in range(0,len(ml.loss_lst),check_after_iter):\n",
    "    lst.append(np.sum(ml.loss_lst[i:i+check_after_iter])/check_after_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fec2ffffad0>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(range(len(lst)), lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1465/2000 [03:51<01:24,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged\n",
      "91.43076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[100], batch_size=100, activation_fn='relu', max_epoch=2000, lr=0.1)\n",
    "ml.fit(X_train, Y_train)\n",
    "ml.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.62307692307692\n"
     ]
    }
   ],
   "source": [
    "ml.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(input_size=784, layers=[100], output_size=26,\n",
       "    batch_size=100, lr=0.1, adaptive_lr=False, activation_fn=relu,\n",
       "    tol=0.0001, n_iter_no_change=10, max_epoch=2000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 73/2000 [00:07<03:05, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged\n",
      "3.830769230769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ml=MLP(layers=[1], batch_size=100, activation_fn='sigmoid', max_epoch=2000, lr=0.1, n_iter_no_change=10000)\n",
    "ml.fit(X_train, Y_train)\n",
    "ml.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    def f(x):\n",
    "        return 1/(1+np.exp(-1*x))\n",
    "    def df(x):\n",
    "        return x*(1-x)\n",
    "#         return sigmoid.f(x)*(1-sigmoid.f(x))\n",
    "    \n",
    "class ReLU:\n",
    "    def f(x):\n",
    "        x_=x.copy()\n",
    "        x_[x_<0]=0\n",
    "        return x_\n",
    "    \n",
    "    def df(x):\n",
    "        x_=x.copy()\n",
    "        x_[x_>0]=1\n",
    "        return x_\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, input_size=784, layers=[100,], output_size=26,\\\n",
    "                 batch_size=100, lr=0.1, adaptive_lr=False, activation_fn='sigmoid',\\\n",
    "                 tol=1e-4, n_iter_no_change=10, max_epoch=100, n_iter_avg=1000):\n",
    "        self.input_size=input_size;\n",
    "        self.layers=layers;\n",
    "        self.output_size=output_size;\n",
    "        self.batch_size=batch_size;\n",
    "        self.lr=lr\n",
    "        self.adaptive_lr=adaptive_lr\n",
    "        \n",
    "        self.activation_fn=activation_fn\n",
    "        if activation_fn=='sigmoid':\n",
    "            self.activation_class = sigmoid\n",
    "        elif activation_fn=='relu':\n",
    "            self.activation_class = ReLU\n",
    "        else:\n",
    "            raise Exception(\"Enter valid activation_fn\")\n",
    "        \n",
    "        self.tol=tol\n",
    "        self.n_iter_no_change=n_iter_no_change \n",
    "        self.max_epoch=max_epoch\n",
    "        \n",
    "        self.layers.insert(0, self.input_size); self.layers.append(self.output_size)\n",
    "        self.n_layers = len(self.layers)\n",
    "        self.n_iter_avg = n_iter_avg\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"\"\"MLP(input_size=%s, layers=%s, output_size=%s,\n",
    "    batch_size=%s, lr=%s, adaptive_lr=%s, activation_fn=%s,\n",
    "    tol=%s, n_iter_no_change=%s, max_epoch=%s)\"\"\"%\\\n",
    "                (self.input_size, self.layers[1:-1], self.output_size,\\\n",
    "                self.batch_size, self.lr, self.adaptive_lr, self.activation_fn,\\\n",
    "                self.tol, self.n_iter_no_change, self.max_epoch)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        np.random.seed(0)\n",
    "        f = self.activation_class.f\n",
    "        df = self.activation_class.df\n",
    "        self.loss_lst = []\n",
    "        iter_no_change=0\n",
    "        itr=0\n",
    "        cnt=0\n",
    "        if self.activation_fn=='sigmoid':\n",
    "            self._intialize_weights(type='glorot-normal')\n",
    "        else:\n",
    "            self._intialize_weights(type='he-normal')\n",
    "        for epoch in tqdm(range(self.max_epoch)):\n",
    "#             itrs_in_one_epoch=math.ceil(X_train.shape[0]/self.batch_size)\n",
    "#             if(epoch%20==0):\n",
    "#                 output = self._forward_prop(X_train)\n",
    "#                 epoch_loss = (0.5/X_train.shape[0])*np.sum((output[-1]-Y_train)**2)\n",
    "#                 self.loss_lst.append(epoch_loss)\n",
    "#                 if(len(self.loss_lst)>=2 and abs(self.loss_lst[-1]-self.loss_lst[-2])<self.tol):\n",
    "#                     iter_no_change+=1\n",
    "#                     if(iter_no_change>=self.n_iter_no_change):\n",
    "#                         cnt+=1\n",
    "#                         if cnt==1:\n",
    "#                             print('Converged in %d epochs'%(epoch))\n",
    "#                             return\n",
    "#                         else:\n",
    "#                             iter_no_change=0\n",
    "#                 else:\n",
    "#                     iter_no_change=0\n",
    "            \n",
    "#             if epoch>=2:\n",
    "#                 loss_diff = abs(np.sum(self.loss_lst[-2*itrs_in_one_epoch:-itrs_in_one_epoch])\\\n",
    "#                                 -np.sum(self.loss_lst[-itrs_in_one_epoch:]))/itrs_in_one_epoch\n",
    "#                 if(loss_diff<self.tol):\n",
    "#                     iter_no_change+=1\n",
    "#                     if(iter_no_change>self.n_iter_no_change):\n",
    "#                         print('Converged in %d epochs'%(epoch))\n",
    "#                         return\n",
    "#                 else:\n",
    "#                     iter_no_change=0\n",
    "            indices = np.random.choice(X_train.shape[0], size=X_train.shape[0])\n",
    "            X_train_ = X_train[indices]\n",
    "            Y_train_ = Y_train[indices]\n",
    "            for i in range(0, X_train.shape[0], self.batch_size):\n",
    "                \n",
    "                X=X_train_[i:i+self.batch_size,:]\n",
    "                Y=Y_train_[i:i+self.batch_size,:]  \n",
    "\n",
    "                # Forward-prop\n",
    "                output = self._forward_prop(X)\n",
    "                \n",
    "#                 loss = (0.5/self.batch_size)*np.sum((output[-1]-Y)**2)\n",
    "                #print(loss)\n",
    "#                 self.loss_lst.append(loss)\n",
    "#                 if abs(self.loss_lst[-1]-self.loss_lst[-2])<=self.tol:\n",
    "#                     iter_no_change+=1\n",
    "#                     if(iter_no_change >=self.n_iter_no_change):\n",
    "#                         return\n",
    "#                 else:\n",
    "#                     iter_no_change=0\n",
    "                    \n",
    "\n",
    "                # Back-Prop\n",
    "                memo, updates = self._back_prop(output, Y)\n",
    "                \n",
    "                # Update\n",
    "                for i in range(self.n_layers-1):\n",
    "                    self.intercepts[i]=self.intercepts[i] - self.lr*np.sum(memo[i],axis=0)\n",
    "                    self.weights[i]=self.weights[i] - self.lr * updates[i]\n",
    "                \n",
    "                itr+=1\n",
    "                \n",
    "#                 check_after_iter=self.n_iter_avg\n",
    "#                 if itr>2*check_after_iter and itr%check_after_iter==0:\n",
    "#                     avg_diff = abs(np.sum(self.loss_lst[-check_after_iter:])-np.sum(self.loss_lst[-2*check_after_iter:-check_after_iter]))/check_after_iter\n",
    "# #                     print(avg_diff)\n",
    "#                     if(avg_diff<self.tol):\n",
    "#                         iter_no_change+=1\n",
    "#                         if(iter_no_change>=self.n_iter_no_change):\n",
    "#                             print('Converged in %d epoches'%(epoch))\n",
    "# #                             return\n",
    "#                     else:\n",
    "#                         iter_no_change=0\n",
    "    \n",
    "    def _forward_prop(self,X):\n",
    "        f = self.activation_class.f\n",
    "        output=[X]\n",
    "        for i in range(self.n_layers-2):\n",
    "            output.append(f(np.dot(output[-1],self.weights[i])+self.intercepts[i]))\n",
    "        output.append(sigmoid.f(np.dot(output[-1],self.weights[-1])+self.intercepts[-1]))\n",
    "        return output\n",
    "    \n",
    "    def _back_prop(self, output, Y):\n",
    "        df=self.activation_class.df\n",
    "        \n",
    "        memo = []\n",
    "        updates = []\n",
    "        curr_memo = (-1/self.batch_size)*(np.multiply(Y-output[-1],sigmoid.df(output[-1])))\n",
    "        memo.insert(0,curr_memo)\n",
    "        curr_update = np.dot(output[-2].T, curr_memo)\n",
    "        updates.insert(0,curr_update)\n",
    "        for i in range(self.n_layers-2, 0, -1):\n",
    "            curr_memo = np.multiply(memo[0].dot(self.weights[i].T),df(output[i]))\n",
    "            memo.insert(0,curr_memo)\n",
    "            curr_update = (output[i-1].T).dot(curr_memo)\n",
    "            updates.insert(0,curr_update)\n",
    "        \n",
    "        return memo, updates\n",
    "    def _intialize_weights(self, type='glorot-normal'):\n",
    "        self.weights=[]\n",
    "        self.intercepts=[]\n",
    "        for i in range(self.n_layers-1):\n",
    "            fan_in = self.layers[i]\n",
    "            fan_out = self.layers[i+1]\n",
    "            #Glorot/Xevier Normal initialization\n",
    "            if type=='glorot-normal':\n",
    "                self.weights.append(np.random.randn(fan_in, fan_out)*(2/(fan_in+fan_out)))\n",
    "                self.intercepts.append(np.random.randn(fan_out)*(2/(fan_in+fan_out)))\n",
    "            elif type=='he-normal':\n",
    "                self.weights.append(np.random.randn(fan_in, fan_out)*np.sqrt(2/fan_in))\n",
    "                self.intercepts.append(np.random.randn(fan_out)*np.sqrt(2/fan_in))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        pred_proba=self.predict_proba(X)\n",
    "        pred=np.zeros(pred_proba.shape, dtype=np.int8)\n",
    "        pred[np.arange(pred.shape[0]), np.argmax(pred_proba, axis=1)] = 1\n",
    "        return pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        f = self.activation_class.f\n",
    "        \n",
    "        pred_proba=X\n",
    "        for i in range(self.n_layers-2):\n",
    "            pred_proba=f(pred_proba@self.weights[i]+self.intercepts[i])\n",
    "        pred_proba=sigmoid.f(pred_proba@self.weights[-1]+self.intercepts[-1])\n",
    "        \n",
    "        return pred_proba\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        return np.log(self.predict_proba(X))\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        pred = self.predict(X)\n",
    "        y_true = np.argmax(Y, axis=1)\n",
    "        y_pred = np.argmax(pred, axis=1)\n",
    "        return (np.sum(y_true==y_pred)*100/y_true.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
